%\documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}

\usepackage{ifthen}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{ulem}
\usepackage{xspace}
\usepackage[scaled=0.8]{beramono}
\usepackage[utf8]{inputenc}

\input{code/style.tex}

\newboolean{showcomments}
\setboolean{showcomments}{true}
\ifthenelse{\boolean{showcomments}}
  {\newcommand{\nb}[2]{
    \fbox{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
   }
   \newcommand{\version}{\emph{\scriptsize$-$Id: main.tex 19055 2008-06-05 11:20:31Z cfbolz $-$}}
  }
  {\newcommand{\nb}[2]{}
   \newcommand{\version}{}
  }

\newcommand\cfbolz[1]{\nb{CFB}{#1}}
\newcommand\toon[1]{\nb{TOON}{#1}}
\newcommand\anto[1]{\nb{ANTO}{#1}}
\newcommand\arigo[1]{\nb{AR}{#1}}
\newcommand\fijal[1]{\nb{FIJAL}{#1}}
\newcommand{\commentout}[1]{}

\newcommand\ie{i.e.,\xspace}
\newcommand\eg{e.g.,\xspace}

\normalem

\let\oldcite=\cite

\renewcommand\cite[1]{\ifthenelse{\equal{#1}{XXX}}{[citation~needed]}{\oldcite{#1}}}

% compressing itemize env, in case we need it
\newenvironment{zitemize}% zero - line spacing itemize environment
   {\begin{list}{--}{
   \setlength{\itemsep}{0 pt}
   \setlength{\parsep}{0 pt}
   \setlength{\topsep} {0 pt} }}% the end stuff
   {\end{list}}

\definecolor{gray}{rgb}{0.5,0.5,0.5}

\begin{document}

\title{XXX in a Tracing JIT Compiler for Efficient Dynamic Languages}

\numberofauthors{4}
\author{
\alignauthor Carl Friedrich Bolz\\
       \affaddr{University of DÃ¼sseldorf}\\
       \affaddr{STUPS Group}\\
       \affaddr{Germany}\\
       \email{cfbolz@gmx.de}
\alignauthor XXX
       \affaddr{XXX}\\
       \email{XXX}
}
\conferenceinfo{ICOOOLPS}{'09 Genova, Italy}
\CopyrightYear{2009}
\crdata{978-1-60558-541-3/09/07}

\maketitle

\category{D.3.4}{Programming Languages}{Processors}[code generation,
incremental compilers, interpreters, run-time environments]

\begin{abstract}


\end{abstract}


%___________________________________________________________________________
\section{Introduction}

One of the hardest parts of implementing a dynamic language efficiently is to
optimize its object model. This is made harder by the fact that many recent
languages such as Python, JavaScript or Ruby have rather complex core object
semantics. For them, implementing just an interpreter is already an arduous
task. Implementing them efficiently with a just-in-time compiler is
nigh-impossible, because or their many corner-cases.

long dream of PE to not have to implement a compiler
new life by meta-tracers, spur, pypy, other things
trace execution of object model and get semantics right, not have to compile
object model

in spur and pypy, this can be improved by the interpreter author by adding
hints, which influence tracer and optimizer

pypy's hints go further than spurs, in this paper we present two very important
ones and show how classical implementation techniques of dynlangs can be
expressed by them



\section{Background}
\label{sec:Background}

\subsection{The PyPy Project}
\label{sect:pypy}

The PyPy project \cite{armin_rigo_pypys_2006} strives to be an environment where
complex dynamic languages can be efficiently implemented. The approach taken
when implement a language with PyPy is to write an interpreter for the language
in \emph{RPython}. RPython is a restricted subset of Python chosen in such a way
that it is possible to perform type inference on it. The interpreters in RPython
can therefore be translated to efficient C code.

A number of languages have been implemented with PyPy, most importantly a full
Python implementation, but also a Prolog interpreter \cite{XXX} and a Smalltalk
VM \cite{XXX}.

This translation to C code adds a number of implementation details into the
final executable that are not present in the interpreter implementation, such as
a garbage collector. The interpreter can therefore be kept free from low-level
implementation details. Another aspect of the final VM that is added
semi-automatically to the generated VM is a tracing JIT compiler.

We call the code that runs on top of an interpreter implemented with PyPy the
\emph{user code} or \emph{user program}.

%___________________________________________________________________________
\subsection{PyPy's Meta-Tracing JIT Compilers}
\label{sect:tracing}

XXX citations
A recently popular approach to JIT compilers is that of tracing JITs. Tracing
JITs record traces of concrete execution paths through the program. Those traces
are therefore linear list of operations, which are optimized and then get turned
into machine code. To be able to do this recording, VMs with a tracing JIT
typically also contain an interpreter. After a user program is started the
interpreter is used until the most important paths through the user program are
turned into machine code.

Because the traces always correspond to a concrete execution they cannot contain
any control flow splits. Therefore they encode the control flow decisions needed
to stay on the trace with the help of \emph{guards}. Those are operations that
check that the assumptions are still true when the trace is later executed with different values.

One disadvantage of tracing JITs which makes them not directly applicable to Pypy,

PyPy's JIT is a meta-tracer \cite{bolz_tracing_2009}. Since we want to re-use
our tracer for a variety of languages, we
don't trace the execution of the user program, but instead trace the execution
of the \emph{interpreter} that is running the program. This means that the traces
don't contain the bytecodes of the language in question, but RPython-level
operations that the interpreter did to execute the program.

On the other hand, the loops that are traced by the tracer are the loops in the
user program. This means that the tracer stops tracing after one iteration of
the loop in the user function that is being considered. At this point, it can
have traced many iterations of the interpreter main loop.

\begin{figure*}
\includegraphics[scale=0.5]{figures/trace-levels}
\caption{The levels involved in tracing}
\label{fig:trace-levels}
\end{figure*}

Figure~\ref{fig:trace-levels} shows a diagram of the process. On the left you
see the levels of execution. The CPU executes the binary of
PyPy's Python interpreter, which consists of RPython functions that have been
compiled first to C, then to machine code. The interpreter runs a Python program written by a
programmer (the user). If the tracer is used, it traces operations on the level
of the interpreter. However, the extent of the trace is determined by the loops
in the user program.

XXX trace makes the object model operations explicit and transparent to the
optimizer

\subsection{Optimizing Traces}
\label{sub:optimizing}

Before sending the trace to the backend to produce actual machine code, it is
optimized.  The optimizer applies a number of techniques to remove or simplify
the operations in the trace. Most of these are well known compiler optimization
techniques, with the difference that it is easier to apply them in a tracing
JIT because it only has to deal with linear traces.  Among the techniques:
%
\begin{itemize}
    \item constant folding
    \item common subexpression elimination
    \item allocation removal \cite{bolz_allocation_2011}
    \item store/load propagation
    \item loop invariant code motion
\end{itemize}

In some places it turns out that if the interpreter author rewrites some parts
of the interpreter with these optimizations in mind the traces that are produced
by the optimizer can be vastly improved.

\subsection{Running Example}
\label{sub:running}

As the running example of this paper we will use a very simple and bare-bones
object model that just supports classes and instances, without any
inheritance or other fancy features. The model has classes, which contain methods.
Instances have a class. Instances have their own attributes (or fields). When looking up an
attribute on an instance, the instances attributes are searched. If the
attribute is not found there, the class' methods are searched.

\begin{figure}
\input{code/interpreter-slow.tex}
\caption{Original Version of a Simple Object Model}
\label{fig:interpreter-slow}
\end{figure}


To implement this object model, we could use the RPython code in
Figure~\ref{fig:interpreter-slow} as part of the interpreter source code.
In this straightforward implementation the methods and attributes are just
stored in dictionaries (hash maps) on the classes and instances, respectively.
While this object model is very
simple it already contains all the hard parts of Python's object model. Both
instances and classes can have arbitrary fields, and they are changeable at
any time.  Moreover, instances can change their class after they have been
created.

When using this object model in
an interpreter, a huge amount of time will be spent doing lookups in these
dictionaries.
Let's assume we trace through code that sums three attributes, such as:

\begin{Verbatim}
inst.getattr("a") + inst.getattr("b") + inst.getattr("c")
\end{Verbatim}

\begin{figure}
\input{code/trace1.tex}
\caption{Trace Through the Object Model}
\label{fig:trace1}
\end{figure}

The trace would look like in Figure~\ref{fig:trace1}. In this example, the
attribute \texttt{a} is found on the instance, but the
attributes \texttt{b} and \texttt{c} are found on the class. The trace indeed contains
five calls to \texttt{dict.get}, which is slow. To make the language efficient
using a tracing JIT, we need to find a way to get rid of these dictionary
lookups somehow. How to achieve this will be topic of
Section~\ref{sec:putting}.



% subsection Running Example (end)

% section Background (end)
%___________________________________________________________________________


\section{Controlling Optimization}

In this section we will describe how to add two hints that allow the
interpreter author to increase the optimization opportunities for constant
folding. If applied correctly these techniques can give really big speedups by
pre-computing parts of what happens at runtime. On the other
hand, if applied incorrectly they might lead to code bloat, thus making the
resulting program actually slower.

For constant folding to work, two conditions need to be met:

\begin{itemize}
    \item the arguments of an operation actually need to all be constant,
    i.e. statically known by the optimizer
    \item the operation needs to be \emph{pure}, i.e. always yield the same result given
    the same arguments.
\end{itemize}

The PyPy JIT generator automatically detects the majority of these conditions.
However, for the cases in which the automatic detection does not work, the
interpreter author can apply \textbf{hints} to improve the optimization
opportunities. There is one kind of hint for both of the conditions above.


\subsection{Where Do All the Constants Come From}

It is worth clarifying what is a ``constant'' in this context.  A variable of
the trace is said to be constant if its value is statically known by the
optimizer.

The simplest example of constants are literal values.  For example, if in the
RPython source code we have a line like \texttt{y = x + 1}, the second operand will
be a constant in the trace.

However, the optimizer can statically know the value of a variable even if it
is not a constant in the original source code. For example, consider the
following fragment of RPython code:
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{if} \PY{n}{x} \PY{o}{==} \PY{l+m+mi}{4}\PY{p}{:}
    \PY{n}{y} \PY{o}{=} \PY{n}{y} \PY{o}{+} \PY{n}{x}
\end{Verbatim}

If the fragment is traced with \texttt{x} being \texttt{4}, the following trace is
produced:
%
\begin{Verbatim}
guard(x == 4)
y = y + x
\end{Verbatim}

In the trace above, the value of \texttt{x} is statically known thanks to the
guard. Remember that a guard is a runtime check. The above trace will run to
completion when \texttt{x == 4}. If the check fails, execution of the trace is
stopped and the interpreter continues to run.

There are cases in which it is useful to turn an arbitrary variable
into a constant value. This process is called \emph{promotion} and it is an old idea
in partial evaluation (it's called ``the trick'' \cite{XXX} there). Promotion is also heavily
used by Psyco \cite{rigo_representation-based_2004} and by all older versions
of PyPy's JIT. Promotion is a technique that only works well in JIT compilers,
in static compilers it is significantly less applicable.

Promotion is essentially a tool for trace specialization. In some places in the
interpreter it would be very useful if a variable were constant, even though it
could have different values in practice. In such a place, promotion is used. The
typical reason to do that is if there is
a lot of computation depending on the value of that variable.

Let's make this more concrete. If we trace a call to the following function:
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{f1}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
    \PY{n}{z} \PY{o}{=} \PY{n}{x} \PY{o}{*} \PY{l+m+mi}{2} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{k}{return} \PY{n}{z} \PY{o}{+} \PY{n}{y}
\end{Verbatim}

We get a trace that looks like this:

\begin{Verbatim}
v1 = x * 2
z = v1 + 1
v2 = z + y
return(v2)
\end{Verbatim}

Observe how the first two operations could be constant-folded if the value of
\texttt{x} were known. Let's assume that the value of \texttt{x} can vary, but does so
rarely, i.e. only takes a few different values at runtime. If this is the
case, we can add a hint to promote \texttt{x}, like this:
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{f2}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{hint}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{promote}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
    \PY{n}{z} \PY{o}{=} \PY{n}{x} \PY{o}{*} \PY{l+m+mi}{2} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{k}{return} \PY{n}{z} \PY{o}{+} \PY{n}{y}
\end{Verbatim}

The meaning of this hint is that the tracer should pretend that \texttt{x} is a
constant
in the code that follows. When just running the code, the function has no
effect, as it simply returns its first argument. When tracing, some extra work
is done. Let's assume that this changed function is traced with
the arguments \texttt{4} and \texttt{8}. The trace will be the same, except for one
operation at the beginning:

\begin{Verbatim}
guard(x == 4)
v1 = x * 2
z = v1 + 1
v2 = z + y
return(v2)
\end{Verbatim}

The promotion is turned into a \texttt{guard} operation in the trace. The guard
captures the value of \texttt{x} as it was at runtime. From the point of view of the
optimizer, this guard is not any different than the one produced by the \texttt{if}
statement in the example above. After the guard, the rest of the trace can
assume that \texttt{x} is equal to \texttt{4}, meaning that the optimizer will turn this
trace into:

\begin{Verbatim}
guard(x == 4)
v2 = 9 + y
return(v2)
\end{Verbatim}

Notice how the first two arithmetic operations were constant folded. The hope is
that the guard is executed quicker than the multiplication and the addition that
was now optimized away.

If this trace is executed with values of \texttt{x} other than \texttt{4}, the guard will
fail, and execution will continue in the interpreter. If the guard fails often
enough, a new trace will be started from the guard. This other trace will
capture a different value of \texttt{x}. If it is e.g. \texttt{2}, then the optimized
trace looks like this:

\begin{Verbatim}
guard(x == 2)
v2 = 5 + y
return(v2)
\end{Verbatim}

This new trace will be attached to the guard instruction of the first trace. If
\texttt{x} takes on even more values, a new trace will eventually be made for all of them,
linking them into a chain. This is clearly not desirable, so we should promote
only variables that don't vary much. However, adding a promotion hint will never produce wrong
results. It might just lead to too much assembler code.

Promoting integers, as in the examples above, is not used that often.
However, the internals of dynamic language interpreters often
have values that are variable but vary little in the context of parts of a user
program. An example would be the types of variables in a user function. Even
though in principle the argument to a Python function could be any Python type,
in practice the argument types tend to not vary often. Therefore it is possible to
promote the types. The next section will present a complete example of how
this works.


\subsection{Declaring New Pure Operations}

In the last section we saw a way to turn arbitrary variables into constants. All
pure operations on these constants can be constant-folded. This works great for
constant folding of simple types, e.g. integers. Unfortunately, in the context of an
interpreter for a dynamic
language, most operations actually manipulate objects, not simple types. The
operations on objects are often not pure and might even have side-effects. If
one reads a field out of a constant reference to an object this cannot
necessarily be folded away because the object can be mutated. Therefore, another
hint is needed.

As an example, take the following class:

\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{A}\PY{p}{(}\PY{n+nb}{object}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{x} \PY{o}{=} \PY{n}{x}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{y} \PY{o}{=} \PY{n}{y}

    \PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{val}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{y} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{compute}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{n}{val}

    \PY{k}{def} \PY{n+nf}{compute}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{x} \PY{o}{*} \PY{l+m+mi}{2} \PY{o}{+} \PY{l+m+mi}{1}
\end{Verbatim}

Tracing the call \texttt{a.f(10)} of some instance of \texttt{A} yields the following
trace (note how the call to \texttt{compute} is inlined):
%
\begin{Verbatim}
x = a.x
v1 = x * 2
v2 = v1 + 1
v3 = v2 + val
a.y = v3
\end{Verbatim}

In this case, adding a promote of \texttt{self} in the \texttt{f} method to get rid of the
computation of the first few operations does not help. Even if \texttt{a} is a
constant reference to an object, reading the \texttt{x} field does not necessarily
always yield the same value. To solve this problem, there is another annotation,
which lets the interpreter author communicate invariants to the optimizer. In
this case, she could decide that the \texttt{x} field of instances of \texttt{A} is
immutable, and therefore \texttt{compute}
is a pure function. To communicate this, there is a \texttt{purefunction} decorator.
If the code in \texttt{compute} should be constant-folded away, we would change the
class as follows:
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{A}\PY{p}{(}\PY{n+nb}{object}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{x} \PY{o}{=} \PY{n}{x}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{y} \PY{o}{=} \PY{n}{y}

    \PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{val}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self} \PY{o}{=} \PY{n}{hint}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{promote}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{y} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{compute}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{n}{val}

    \PY{n+nd}{@purefunction}
    \PY{k}{def} \PY{n+nf}{compute}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{x} \PY{o}{*} \PY{l+m+mi}{2} \PY{o}{+} \PY{l+m+mi}{1}
\end{Verbatim}

Now the trace will look like this:
%
\begin{Verbatim}
guard(a == 0xb73984a8)
v1 = compute(a)
v2 = v1 + val
a.y = v2
\end{Verbatim}

Here, \texttt{0xb73984a8} is the address of the instance of \texttt{A} that was used
during tracing. The call to \texttt{compute} is not inlined, so that the optimizer
has a chance to see it. Since \texttt{compute} function is marked as pure, and its
argument
is a constant reference, the call will be removed by the optimizer. The final
trace looks like this:
%
\begin{Verbatim}
guard(a == 0xb73984a8)
v2 = 9 + val
a.y = v2
\end{Verbatim}

(assuming that the \texttt{x} field's value is \texttt{4}).

On the one hand, the \texttt{purefunction} annotation is very powerful. It can be
used to constant-fold arbitrary parts of the computation in the interpreter.
However, the annotation also gives you ample opportunity to mess things up. If a
function is annotated to be pure, but is not really, the optimizer can produce
subtly wrong code. Therefore, a lot of care has to be taken when using this
annotation.


\subsubsection{Observably Pure Functions}

Why can't we simply write an analysis to find out that the \texttt{x} fields of the
\texttt{A} instances is immutable and deduce that \texttt{compute} is a pure function,
since it only reads the \texttt{x} field and does not have side effects? This might
be possible in this particular case, but in practice the functions that are
annotated with the \texttt{purefunction} decorator are usually more complex.
The easiest example for this is that of a function that uses memoization to
cache its results. If you analyze this function, it looks like the function has
side effects, because it changes the memoizing dictionary. However, because this side
effect is not externally visible, the function from the outside is pure. This is
a property that is not easily detectable by analysis. Therefore, the purity
of this function needs to be annotated.



\subsubsection{Immutable Fields}

One of the most common cases of pure functions is reading immutable
values out of objects. Since this is so common, we have special syntactic sugar
for it. A RPython class can have a class attribute \texttt{\_immutable\_fields\_} set to
a list of strings, listing the fields that cannot be changed. This is equivalent
to using getters and annotating them with \texttt{purefunction}.



\subsection{Conclusion}

In this section we presented two more hints that can be used in the source code
of the interpreter. They are used to influence what the optimizer does with the
trace. The examples given here are a bit too small, the next
section gives a worked-out example that puts all the pieces together.

%___________________________________________________________________________

\section{Putting It All Together}

In this section we describe how the simple object model from
Section~\ref{sub:running} can be made efficient using the hints described in the
previous the section. The object model there is typical for many current
dynamic languages (such as Python, Ruby and JavaScript) as it relies heavily on
hash-maps to implement its objects.

%___________________________________________________________________________

\subsection{Making Instance Attributes Faster Using Maps}

The first step in making \texttt{getattr} faster in our object model is to optimize
away the dictionary lookups on the instances. The hints we have looked at in the
two previous sections don't seem to help with the current object model. There is
no pure function to be seen, and the instance is not a candidate for promotion,
because there tend to be many instances.

This is a common problem when trying to apply hints. Often, the interpreter
needs a small rewrite to expose the pure functions and nearly-constant objects
that are implicitly there. In the case of instance fields this rewrite is not
entirely obvious. The basic idea is as follows. In theory instances can have
arbitrary fields. In practice however many instances share their layout (i.e.
their set of keys) with many other instances.

Therefore it makes sense to factor the layout information out of the instance
implementation into a shared object, called the \emph{map}. Maps are a well-known
technique to efficiently implement instances and come from the SELF project
\cite{XXX}. They are also used by many JavaScript implementations such as V8.
The rewritten \texttt{Instance} class using maps can be seen in
Figure~\ref{fig:maps}.

\begin{figure}
\input{code/map.tex}
\caption{Simple Object Model With Maps}
\label{fig:maps}
\end{figure}

In this implementation instances no longer use dictionaries to store their fields. Instead, they have a
reference to a map, which maps field names to indexes into a storage list. The
storage list contains the actual field values. The maps are shared between
objects with the same layout. Therefore they have to be immutable, which means
that their \texttt{getindex} method is a pure function. When a new attribute is added
to an instance, a new map needs to be chosen, which is done with the
\texttt{add\_attribute} method on the previous map (which is also pure). Now that we have
introduced maps, it is safe to promote the map everywhere, because we assume
that the number of different instance layouts is small.

With this changed instance implementation, the trace we had above changes to the
following that of see Figure~\ref{fig:trace2}. There \texttt{0xb74af4a8} is the
memory address of the \texttt{Map} instance that has been promoted. Operations
that can be optimized away are grayed out.

The calls to \texttt{Map.getindex} can be optimized away, because they are calls to
a pure function and they have constant arguments. That means that \texttt{index1/2/3}
are constant and the guards on them can be removed. All but the first guard on
the map will be optimized away too, because the map cannot have changed in
between. This trace is already much better than
the original one. Now we are down from five dictionary lookups to just two.

\begin{figure}
\input{code/trace2.tex}
\caption{Unoptimized Trace After the Introduction of Maps}
\label{fig:trace2}
\end{figure}




%___________________________________________________________________________

\subsection{Versioning of Classes}

Instances were optimized making the assumption that the total number of
different instance layouts is small compared to the number of instances. For classes we
will make an even stronger assumption. We simply assume that it is rare for
classes to change at all. This is not totally reasonable (sometimes classes contain
counters or similar things) but for this simple example it is good enough.

What we would really like is if the \texttt{Class.find\_method} method were pure.
But it cannot be, because it is always possible to change the class itself.
Every time the class changes, \texttt{find\_method} can potentially return a
new value.

Therefore, we give every class a version number, which is changed every time a
class gets changed (i.e., the content of the \texttt{methods} dictionary changes).
This means that the result of \texttt{methods.get()} for a given \texttt{(name,
version)} pair will always be the same, i.e. it is a pure operation.  To help
the JIT to detect this case, we factor it out in a helper method which is
explicitly marked as \texttt{@purefunction}. The refactored \texttt{Class} can
be seen in Figure~\ref{fig:version}

\begin{figure}
\input{code/version.tex}
\caption{Versioning of Classes}
\label{fig:version}
\end{figure}

What is interesting here is that \texttt{\_find\_method} takes the \texttt{version}
argument but it does not use it at all. Its only purpose is to make the call
pure, because when the version number changes, the result of the call might be
different than the previous one.

\begin{figure}
\input{code/trace4.tex}
\caption{Unoptimized Trace After Introduction of Versioned Classes}
\label{fig:trace4}
\end{figure}

The trace with this new class implementation can be seen in
Figure~\ref{fig:trace4}.
The calls to \texttt{Class.\_find\_method} can now be optimized away, also the
promotion of the class and the version, except for the first one. The final
optimized trace can be seen in Figure~\ref{fig:trace5}.

\begin{figure}
\input{code/trace5.tex}
\caption{Optimized Trace After Introduction of Versioned Classes}
\label{fig:trace5}
\end{figure}

The index \texttt{0} that is used to read out of the \texttt{storage} array is the result
of the constant-folded \texttt{getindex} call.
The constants \texttt{41} and \texttt{17} are the results of the folding of the
\texttt{\_find\_method`} calls. This final trace is now very good. It no longer performs any
dictionary lookups. Instead it contains several guards. The first guard
checks that the map is still the same. This guard will fail if the same
code is executed with an instance that has another layout. The second guard
checks that the class of \texttt{inst} is still the same. It will fail if the trace is
executed with an instance of another class. The third guard checks that the
class did not change since the trace was produced. It will fail if somebody
calls the \texttt{change\_method} method on the class.


%___________________________________________________________________________

\subsection{Real-World Considerations}

The techniques used above for the simple object model are used for the object
model of PyPy's Python interpreter too. Since Python's object model is
considerably more complex, some additional work needs to be done.

The first problem that needs to be solved is that Python supports (multiple)
inheritance. Therefore looking up a method in a class needs to consider all the
classes in the
whole method resolution order. This makes the versioning of classes more
complex. If a class is changed its version changes. At the same time, the
versions of all the classes inheriting from it need to be changed as well,
recursively. This makes class changes expensive, but they should be rare.  On the
other hand, a method lookup in a complex class hierarchy is as optimized in the
trace as in our object model here.

A downside of the versioning of classes that we haven't yet fixed in PyPy, is
that some classes \emph{do} change a lot. An example would be a class that keeps a
counter of how many instances have been created so far. This is very slow right
now, but we have ideas about how to fix it in the future.

Another optimization is that in practice the shape of an instance is correlated
with its class. In our code above, we allow both to vary independently.
In PyPy's Python interpreter we act somewhat more cleverly. The class of
an instance is not stored on the instance itself, but on the map. This means
that we get one fewer promotion (and thus one fewer guard) in the trace,
because the class doesn't need to
be promoted after the map has been.


%___________________________________________________________________________

\subsection{More General Patterns}

The techniques we used above to make instance and class lookups faster are
applicable in more general cases than the one we developed them for. A more
abstract view of maps is that of splitting a data-structure into a part that
changes slowly, and a part that changes quickly. In the concrete example of maps
we split the original dictionary into the map (the slow-changing part) and the
storage array (the quick-changing part). All the computation on the
slow-changing part can be constant-folded during tracing so that only the
manipulation of the quick-changing part remains.

Similarly, versions can be used to constant-fold arbitrary functions of large data
structures. The version needs to be updated carefully every time the result of
this function can change. Therefore this is useful only if the data structure is
expected to change slowly.


%___________________________________________________________________________

\subsection{Conclusion}

In this section we saw how to use \texttt{purefunction} and \texttt{promote} to make a
small but still relevant dynamic object model no longer use any dictionary lookups
after tracing. Instead a number of guards are inserted into the
trace to check whether the assumptions about the objects are still true. This
makes operations on objects seriously faster.

\section{Evaluation}
\label{sect:evaluation}

\section{Related Work}

\section{Conclusion and Next Steps}

\section*{Acknowledgements}

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}
