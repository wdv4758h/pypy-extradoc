
\documentclass{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage[utf8]{inputenc}

\usepackage{amsmath}


\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{ICOOOLPS workshop 2014}{July 28th, 2014, Uppsala, Sweden}
\copyrightyear{2014}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish,
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers,
                                  % short abstracts)

%% \titlebanner{banner above paper title}        % These are ignored unless
%% \preprintfooter{short description of paper}   % 'preprint' option specified.

\title{The Way Forward in Parallelizing Dynamic Languages}
\subtitle{Position Paper, ICOOOLPS'14}

\authorinfo{Remi Meier}
           {Department of Computer Science\\ ETH ZÃ¼rich}
           {remi.meier@inf.ethz.ch}
\authorinfo{Armin Rigo}
           {www.pypy.org}
           {arigo@tunes.org}

\maketitle

\begin{abstract}
This is the text of the abstract.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
%% \terms
%% term1, term2

\keywords
transactional memory, dynamic languages, parallelism, global interpreter lock

\section{Introduction}
In a world where computers get more and more cores and single-thread
performance increases less and less every year, many dynamic languages
have a problem. While there is certainly a lot of popularity around
languages like Python and Ruby, their ability to make use of multiple
cores is somewhat limited. For ease of implementation they chose to
use a single, global interpreter lock (GIL) to synchronize the
execution of code in multiple threads. While this is a
straight-forward way to eliminate synchronization issues in the
interpreter, it prevents parallel execution. Code executed in multiple
threads will be serialized over this GIL so that only one thread can
execute at a time.

There exist several solutions and work-arounds to remove or avoid the
GIL in order to benefit from multiple cores. We are going to discuss
several of them and try to find the best way forward. The first
approach uses fine-grained locking to replace the single GIL. Then
there are shared-nothing models that use for example multiple
processes with multiple interpreters and explicit message
passing. Finally, one can also directly replace the GIL with
transactional memory (TM), either software-based (STM) or
hardware-based (HTM).

The approach that wins in the end should perform similarly for
single-threaded execution as compared to the GIL and be able to
execute code in parallel on multiple cores. Furthermore, we will also
take into account the compatibility to existing code that already uses
threads for concurrency, as well as the changes that are required to
the interpreter itself.

These requirements are not easy to meet. We argue that STM is the
overall winner. While it has a big performance problem currently, it
gets more points in all the other categories. We think that it is the
only solution that also provides a better synchronization mechanism to
the application in the form of atomic blocks.

%% \subsection{Issue}
%% The issue that we want to discuss is how to efficiently support
%% multi-core parallel execution of code in dynamic languages that were
%% designed with GIL semantics in mind.

%% Furthermore, a solution to this problem should also bring better
%% synchronization mechanism with it...

%% (supporting (large) atomic blocks for synchronization)

%% \subsection{Our Position}
%% Current solutions for replacing the GIL include STM, HTM, and
%% fine-grained locking. STM is usually too slow, HTM very limited, and
%% locking suffers from complexity that makes it hard to implement
%% correctly. We argue that the best way forward is still STM and that
%% its performance problem can be solved.

%% Current solutions like STM, HTM, and fine-grained locking are slow, hard
%% to implement correctly, and don't fit the specific problems of dynamic
%% language.  STM is the best way forward but has bad performance, so we
%% fix that.

\section{Discussion}

\paragraph{dynamic language VM problems}

- high allocation rate (short lived objects)\\
- (don't know anything about the program that runs until it actually runs: arbitrary atomic block size)


\subsection{Why is there a GIL?}
The GIL is a very simple synchronization mechanism for supporting
multi-threading in the interpreter. The basic guarantee is that the
GIL may only be released in-between bytecode instructions. The
interpreter can thus rely on complete isolation and atomicity of these
instructions. As a consequence, applications can rely on certain
operations to be atomic. While this is probably not a good idea,
it is used in practice. A solution replacing the GIL should therefore
uphold these guarantees, while preferably also be as easily
implementable as a GIL for the interpreter.

The GIL also allows for easy integration with external C libraries that
do not need to be thread-safe. For the duration of the calls, we
simply do not release the GIL. External libraries that are explicitly
thread-safe can voluntarily release the GIL themselves in order to
still provide some parallelism. This is done for example for
potentially long I/O operations. Consequently, I/O-bound,
multi-threaded applications can actually parallelize to some
degree. Again, a potential solution should be able to integrate with
external libraries with similar ease. We will however focus our
argumentation more on running code in the interpreted language in
parallel, not the external C calls.

Since the GIL is mostly an implementation detail of the interpreter,
it is not exposed to the application running on top of it. To
synchronize memory accesses in applications using threads, the
state-of-the-art still means explicit locking everywhere. It is well
known that using locks for synchronization is not easy.  They are
non-composable, have overhead, may deadlock, limit scalability, and
overall add a lot of complexity. For a better parallel programming
model for dynamic languages, we propose another, well-known
synchronization mechanism called \emph{atomic blocks}.

Atomic blocks are composable, deadlock-free, higher-level and expose
useful atomicity and isolation guarantees to the application for a
series of instructions.  Interpreters using using a GIL can simply
guarantee that the GIL is not released during the execution of the
atomic block. Of course, this still means that no two atomic blocks
can execute in parallel or even concurrently. Potential solutions
that provide a good way to implement atomic blocks are therefore
preferable.



\subsection{Potential Solutions}

The list of criterias for evaluating potential solutions is as follows:
\begin{itemize}
\item Performance
\item Changes required to existing applications
\item Better synchronization options for applications (e.g. atomic blocks)
\item Ease of implementation (interpreter-level)
\item Integration with external libraries
\end{itemize}

\subsubsection*{fine-grained locking}

- support of atomic blocks?\\
- hard to get right (deadlocks, performance, lock-granularity)\\
- very hard to get right for a large language\\
- hard to retro-fit, as all existing code assumes GIL semantics\\
- (there are some semantic differences, right? not given perfect lock-placement, but well)
( http://www.jython.org/jythonbook/en/1.0/Concurrency.html )

\subsubsection*{Shared-nothing / multiple processes}

- often needs major restructuring of programs (explicit data exchange)\\
- sometimes communication overhead is too large\\
- shared memory is a problem, copies of memory are too expensive

\subsubsection*{Transactional Memory}
\paragraph{HTM}

- false-sharing on cache-line level\\
- limited capacity (caches, undocumented)\\
- random aborts (haswell)\\
- generally: transaction-length limited (no atomic blocks)

\paragraph{STM}

- overhead (100-1000\%) (barrier reference resolution, kills performance on low \#cpu)
(FastLane: low overhead, not much gain)\\
- unlimited transaction length (easy atomic blocks)

\section{The Way Forward}
possible solution:\\
- use virtual memory paging to somehow lower the STM overhead\\
- tight integration with GC and jit?


\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}


\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices
