
\documentclass{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{hyperref}
\usepackage{amsmath}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{ICOOOLPS workshop 2014}{July 28th, 2014, Uppsala, Sweden}
\copyrightyear{2014}
%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish,
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers,
                                  % short abstracts)

%% \titlebanner{banner above paper title}        % These are ignored unless
%% \preprintfooter{short description of paper}   % 'preprint' option specified.

\title{A Way Forward in Parallelising Dynamic Languages}
\subtitle{Position Paper, ICOOOLPS'14}

\authorinfo{Remigius Meier}
           {Department of Computer Science\\ ETH Zürich}
           {remi.meier@inf.ethz.ch}
\authorinfo{Armin Rigo}
           {www.pypy.org}
           {arigo@tunes.org}

\maketitle

\begin{abstract}
  Dynamic languages became very popular in recent years. At some
  point, the need for concurrency arose, and many of them made the
  choice to use a single global interpreter lock (GIL) to synchronise
  the interpreter in a multithreading scenario. This choice however
  makes it impossible to actually run code in parallel.

  Here we want to compare different approaches to replacing the GIL
  with a technology that allows parallel execution. We look at
  fine-grained locking, shared-nothing, and transactional memory (TM)
  approaches. We argue that software-based TM systems are the most
  promising, especially since they also enable the introduction of
  atomic blocks as a better synchronisation mechanism in the language.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
%% \terms
%% term1, term2

\keywords
transactional memory, dynamic languages, parallelism, global interpreter lock

\section{Introduction}
In a world where computers get more and more cores and single-thread
performance increases less and less every year, many dynamic languages
have a problem. While there is certainly a lot of popularity around
languages like Python and Ruby, their ability to make use of multiple
cores is somewhat limited. For ease of implementation they chose to
use a single, global interpreter lock (GIL) to synchronise the
execution of code in multiple threads. While this is a
straight-forward way to eliminate synchronisation issues in the
interpreter, it prevents parallel execution. Code executed in multiple
threads will be serialised over this GIL so that only one thread can
execute at a time.

There exist several solutions and workarounds to remove or avoid the
GIL in order to benefit from multiple cores. We are going to discuss
several of them and try to find the best way forward. The first
approach uses fine-grained locking to replace the single GIL. Then
there are shared-nothing models that use for example multiple
processes with multiple interpreters and explicit message
passing. Finally, one can also directly replace the GIL with
transactional memory (TM), either software-based (STM) or
hardware-based (HTM).

The approach that wins in the end should perform similarly for
single-threaded execution as compared to the GIL and be able to
execute code in parallel on multiple cores. Furthermore, we will also
take into account the compatibility to existing code that already uses
threads for concurrency, as well as the changes that are required to
the interpreter itself.

These requirements are not easy to meet. We argue that STM is the
overall winner. While it has a big performance problem currently, it
gets more points in all the other categories. We think that it is the
only solution that also provides a better synchronisation mechanism to
the application in the form of atomic blocks.

%% \subsection{Issue}
%% The issue that we want to discuss is how to efficiently support
%% multi-core parallel execution of code in dynamic languages that were
%% designed with GIL semantics in mind.

%% Furthermore, a solution to this problem should also bring better
%% synchronisation mechanism with it...

%% (supporting (large) atomic blocks for synchronisation)

%% \subsection{Our Position}
%% Current solutions for replacing the GIL include STM, HTM, and
%% fine-grained locking. STM is usually too slow, HTM very limited, and
%% locking suffers from complexity that makes it hard to implement
%% correctly. We argue that the best way forward is still STM and that
%% its performance problem can be solved.

%% Current solutions like STM, HTM, and fine-grained locking are slow, hard
%% to implement correctly, and don't fit the specific problems of dynamic
%% language.  STM is the best way forward but has bad performance, so we
%% fix that.

\section{Discussion}

In this section we examine the approaches and highlight their
advantages and disadvantages.
%% \paragraph{dynamic language VM problems}
%% XXX:
%% - high allocation rate (short lived objects)\\
%% - (don't know anything about the program that runs until it actually runs: arbitrary atomic block size)


\subsection{Why is there a GIL?}
The GIL is a very simple synchronisation mechanism for supporting
multithreading in an interpreter. The basic guarantee is that the GIL
may only be released in between bytecode instructions. The interpreter
can thus rely on complete isolation and atomicity of these
instructions. Additionally, it provides the application with a
sequential consistency model\cite{lamport79}. As a consequence,
applications can rely on certain operations to be atomic and that they
will always be executed in the order in which they appear in the
code. While depending on this may not always be a good idea, it is
done in practice. A GIL-replacement should therefore uphold these
guarantees, while preferably also be as easily implementable as a GIL
for the interpreter. The latter can be especially important since
many of these languages are developed and maintained by very large
open-source communities, which are not easy to coordinate.

The GIL also allows for easy integration with external C libraries that
may not be thread-safe. For the duration of the calls, we
simply do not release the GIL. External libraries that are explicitly
thread-safe can voluntarily release the GIL themselves in order to
still provide some parallelism. This is done for example for
potentially long I/O operations. Consequently, I/O-bound,
multithreaded applications can actually parallelise to some
degree. Again, a potential solution should be able to integrate with
external libraries with similar ease. We will however focus our
argumentation more on running code in the interpreted language in
parallel, not the external C calls.

Since the GIL is mostly an implementation detail of the interpreter,
it is not exposed to the application running on top of it. To
synchronise memory accesses in applications using threads, the
state-of-the-art still means explicit locking everywhere. It is well
known that using locks for synchronisation is not easy.  They are
non-composable, have overhead, may deadlock, limit scalability, and
overall add a lot of complexity. For a better parallel programming
model for dynamic languages, we propose another, well-known
synchronisation mechanism called \emph{atomic blocks}.

Atomic blocks are composable, deadlock-free, higher-level and expose
useful atomicity and isolation guarantees to the application for a
series of instructions.  Interpreters using a GIL can simply guarantee
that the GIL is not released during the execution of the atomic
block. Of course, this still means that no two atomic blocks can
execute in parallel or even concurrently. Potential solutions that
provide a good way to implement atomic blocks are therefore
preferable.



\subsection{Potential Solutions}

For the discussion we define a set of criteria to evaluate the
multiple potential solutions for removing or avoiding the GIL and its
limitations:

\begin{description}
\item[Performance:] How well does the approach perform compared to the
  GIL on single and multiple threads?
\item[Existing applications:] How big are the changes required to
  integrate with and parallelise existing applications?
\item[Better synchronisation:] Does the approach enable better
  synchronisation mechanisms for applications (e.g. atomic blocks)?
\item[Implementation:] How difficult is it to implement the approach
  in the interpreter?
\item[External libraries:] Does the approach allow for easy
  integration of external libraries?
\end{description}


\subsubsection{Fine-Grained Locking}

The first obvious candidate to replace the GIL is to use multiple
locks instead of a single global lock. By refining the granularity of
the locking approach, we gain the ability to run code that does not
access the same objects in parallel. What we lose instead is the
simplicity of the GIL approach. With every additional lock, the
likeliness of deadlocks grows, as well as the overhead that acquiring
and releasing locks produces. The former means that sometimes it is
necessary to fall back to less fine-grained locking, preventing some
potential parallelism in order to keep the complexity manageable.
The latter means that we lose a bit of performance in the
single-threaded case compared to the GIL, which requires much less
acquire-release operations.

Jython\cite{webjython} is one project that implements an
interpreter for Python on the JVM\footnote{Java Virtual Machine} and
that uses fine-grained locking to correctly synchronise the
interpreter. For a language like Python, one needs quite a few,
carefully placed locks. Since there is no central location, the
complexity of the implementation is quite a bit greater compared to
using a GIL. Integrating external, non-thread-safe libraries should
however be very simple too. One could simply use one lock per library
to avoid this issue.

In the end, fine-grained locking can transparently replace the GIL
and therefore parallelise existing applications, generally without any
changes\footnote{There are rare cases where not having atomic
bytecodes actually changes the semantics.}
It does however not provide a better synchronisation
mechanism to the application like e.g. atomic blocks.

%% - support of atomic blocks?\\
%% - hard to get right (deadlocks, performance, lock-granularity)\\
%% - very hard to get right for a large language\\
%% - hard to retro-fit, as all existing code assumes GIL semantics\\
%% - (there are some semantic differences, right? not given perfect lock-placement, but well)
%% ( http://www.jython.org/jythonbook/en/1.0/Concurrency.html )

\subsubsection{Shared-Nothing}

There are also approaches that work around the GIL instead of trying
to replace it. If an application can be split into completely
independent parts that only very rarely need to share anything, or
only do so via an external program like a database, then it is
sensible to have one GIL per independent part. As an example of such
an approach we look at the
\emph{multiprocessing}\footnote{https://docs.python.org/2/library/multiprocessing.html}
module of Python. In essence, it uses process-forking to provide the
application with multiple interpreters that can run in parallel.
Communication is then done explicitly through pipes.

Obviously not every application fits well into this model and its
applicability is therefore quite limited. Performance is good as
long as the application does not need to communicate a lot, because
inter-process communication is relatively expensive. Also the
implementation of this approach is very cheap since one can
actually take an unmodified GIL-supported interpreter and run
multiple of them in parallel. That way, we also inherit the
easy integration of external libraries without any changes.
While the model of explicit communication is often seen as a
superior way to synchronise concurrent applications because
of its explicitness, it does not actually introduce a better
synchronisation mechanism for applications.

%% - often needs major restructuring of programs (explicit data exchange)\\
%% - sometimes communication overhead is too large\\
%% - shared memory is a problem, copies of memory are too expensive


\subsubsection{Transactional Memory}
Transactional memory (TM) can be used as a direct replacement for a
single global lock. Transactions provide the same atomicity and
isolation guarantees as the GIL provides for the execution of bytecode
instructions. So instead of acquiring and releasing the GIL between
these instructions, this approach runs the protected instructions
inside transactions.

TM can be implemented in software (STM) or in hardware (HTM). There
are also hybrid approaches, which combine the two. We count these
hybrid approaches as STM, since they usually provide the same
capabilities as software-only approaches but with different
performance characteristics. We will now first look at HTM, which
recently gained a lot of popularity by its introduction in common
desktop CPUs from Intel (Haswell generation).

\paragraph{HTM} provides us with transactions like any TM system does.
It can be used as a direct replacement for the GIL\cite{nicholas06,odaira14,fuad10}. However, as is
common with hardware-only solutions, there are quite a few limitations
that can not be lifted easily. For this comparison, we look at the
implementation of Intel in recent Haswell generation CPUs.

HTM in these CPUs works on the level of caches. This has a few
consequences like false-sharing on the cache-line level, and most
importantly it limits the amount of memory that can be accessed within
a transaction. This transaction-length limitation makes it necessary
to have a fallback in place in case this limit is reached. In recent
attempts, the usual fallback is the GIL\cite{odaira14,fuad10}. In our
experiments, the current generation of HTM proved to be very fragile
and thus needing the fallback very often. Consequently, scalability
suffered a lot from this.

The performance of HTM is pretty good as it does not introduce much
overhead ($<40\%$ overhead\cite{odaira14}). And it can transparently
parallelise existing applications to some degree. The implementation
is very straight-forward because it directly replaces the GIL in a
central place. HTM is also directly compatible with any external
library that needs to be integrated and synchronised for use in
multiple threads. The one thing that is missing is support for a
better synchronisation mechanism for the application. It is not
possible in general to expose the hardware-transactions to the
application in the form of atomic blocks because that would require
much longer transactions.

%% - false-sharing on cache-line level\\
%% - limited capacity (caches, undocumented)\\
%% - random aborts (haswell)\\
%% - generally: transaction-length limited (no atomic blocks)

\paragraph{STM} provides all the same benefits as HTM except for its
performance.  It is not unusual for the overhead introduced by STM to
be between 100\% to even 1000\% \cite{cascaval08,drago11}. While STM
systems often scale very well to a big number of threads and
eventually overtake the single-threaded execution, they often provide
no benefits at all for low numbers of threads (1-8). There are some
attempts \cite{warmhoff13,spear09} that can reduce the overhead a lot,
but scale badly or only for certain workloads. Often the benefits
on more than one thread are too little in real world applications.

However, STM compared to HTM does not suffer from the same restricting
limitations. Transactions can be arbitrarily long.  This makes it
possible to actually expose transactions to the application in the
form of atomic blocks. This is the only approach that enables a better
synchronisation mechanism than locks for applications \emph{and} still
parallelises when using it. We think this is a very important point
because it not only gives dynamic languages the ability to parallelise
(already commonplace in most other languages), but also pushes
parallel programming forward. Together with sequential consistency it
provides a lot of simplification for parallel applications.

While one can argue that STM requires the insertion of read and write
barriers in the whole program, this can be done automatically and
locally by a program transformation\cite{felber07}. There are attempts
to do the same for fine-grained locking\cite{bill06} but they require
a whole program analysis since locks are inherently non-composable.
The effectiveness of these approaches is doubtful in our use case,
since we execute bytecode instructions in any order defined by a
script only known at runtime. This makes it close to impossible to
order locks consistently or to know in advance which locks a
transaction will need.

%% - overhead (100-1000\%) (barrier reference resolution, kills performance on low \#cpu)
%% (FastLane: low overhead, not much gain)\\
%% - unlimited transaction length (easy atomic blocks)



\section{The Way Forward}

\begin{table*}[h]
  \centering
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    & \textbf{GIL} & \textbf{Fine-grained locking}
    & \textbf{Shared-nothing} & \textbf{HTM} & \textbf{STM}\\
    \hline
    Performance (single threaded) & ++   & +  & ++   & ++ & -{-} \\
    \hline
    Performance (multithreaded)   & -{-} & +  & +    & +  & +    \\
    \hline
    Existing applications         & ++   & ++ & -{-} & ++ & ++   \\
    \hline
    Better synchronisation        & -    & -  & -    & -  & ++   \\
    \hline
    Implementation                & ++   & -  & ++   & ++ & ++   \\
    \hline
    External libraries            & ++   & ++ & ++   & ++ & ++   \\
    \hline
  \end{tabular}
  \caption{Comparison between the approaches (-{-}/-/o/+/++)}
  \label{tab:comparison}
\end{table*}


Following the above argumentation for each approach we assembled a
general overview in Table \ref{tab:comparison}. The general picture is
everything else than clear. It looks like HTM may be a good solution
to replace the GIL in the near future. Current implementations are
however far too limiting and do not provide good scaling.

Allowing for parallel execution just means that dynamic languages
catch up to all other languages that already provide real
parallelism. This is why we think that only the STM approach is a
viable solution in the long-term. It provides the application with a
simple memory model (sequential consistency) and a composable way to
synchronise memory accesses using atomic blocks.

Unfortunately, STM has a big performance problem. Particularly, for
our use case there is not much static information available since we
are executing a program only known at runtime. Additionally, replacing
the GIL means running every part of the application in transactions,
so there is not much code that can run outside and that can be
optimised better. The performance of the TM system is vital.

One way to get more performance is to develop STM systems that make
better use of low-level features in existing OS kernels.  We are
currently working on a STM system that makes use of several such
features like virtual memory and memory segmentation.  We further
tailor the system to the discussed use case which gives us an
advantage over other STM systems that are more general. With this
approach, initial results suggest that we can keep the overhead of STM
below 50\%. A hybrid TM system, which also uses HTM to accelerate
certain tasks, looks like a very promising direction of research
too. We believe that further work to reduce the overhead of STM is
very worthwhile.




%% possible solution:\\
%% - use virtual memory paging to somehow lower the STM overhead\\
%% - tight integration with GC and jit?


%% \appendix
%% \section{Appendix Title}

%% This is the text of the appendix, if you need one.

\acks
We would like to thank Maciej Fijalkowski and Carl Friedrich Bolz for
their valuable inputs and the many fruitful discussions.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem{webjython}
  The Jython Project, \url{www.jython.org}

\bibitem{odaira14}
  Odaira, Rei, Jose G. Castanos, and Hisanobu Tomari.  "Eliminating
  global interpreter locks in Ruby through hardware transactional
  memory."  \emph{Proceedings of the 19th ACM SIGPLAN symposium on
    Principles and practice of parallel programming.} ACM, 2014.

\bibitem{warmhoff13}
  Wamhoff, Jons-Tobias, et al. "FastLane: improving performance of
  software transactional memory for low thread counts."
  \emph{Proceedings of the 18th ACM SIGPLAN symposium on Principles
    and practice of parallel programming.} ACM, 2013.

\bibitem{drago11}
  Dragojević, Aleksandar, et al. "Why STM can be more than a research
  toy." \emph{Communications of the ACM} 54.4 (2011): 70-77.

\bibitem{cascaval08}
  Cascaval, Calin, et al. "Software transactional memory: Why is it
  only a research toy?." \emph{Queue} 6.5 (2008): 40.

\bibitem{nicholas06}
  Nicholas Riley and Craig Zilles. 2006. Hardware transactional memory
  support for lightweight dynamic language evolution. \emph{In
    Companion to the 21st ACM SIGPLAN symposium on Object-oriented
    programming systems, languages, and applications} (OOPSLA
  '06). ACM, New York, NY, USA

\bibitem{fuad10}
  Fuad Tabba. 2010. Adding concurrency in python using a commercial
  processor's hardware transactional memory support. \emph{SIGARCH
  Comput. Archit. News 38}, 5 (April 2010)

\bibitem{felber07}
  Felber, Pascal, et al. "Transactifying applications using an open
  compiler framework." \emph{TRANSACT}, August (2007): 4-6.

\bibitem{bill06}
  Bill McCloskey, Feng Zhou, David Gay, and Eric
  Brewer. 2006. Autolocker: synchronization inference for atomic
  sections. \emph{In Conference record of the 33rd ACM SIGPLAN-SIGACT
  symposium on Principles of programming languages (POPL '06)}. ACM,
  New York, NY, USA

\bibitem{spear09}
  Spear, Michael F., et al. "Transactional mutex locks." \emph{SIGPLAN
    Workshop on Transactional Computing.} 2009.

\bibitem{lamport79}
  Lamport, Leslie. "How to make a multiprocessor computer that
  correctly executes multiprocess programs." \emph{Computers, IEEE
    Transactions} on 100.9 (1979): 690-691.


\end{thebibliography}


\end{document}
