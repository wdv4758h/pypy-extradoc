\section{Benchmarks}
\label{sec:benchmarks}

In section \ref{sec:tlc-properties}, we saw that TLC provides most of the
features that usually make dynamically typed language so slow, such as
\emph{stack-based interpreter}, \emph{boxed arithmetic} and \emph{dynamic lookup} of
methods and attributes.

In the following sections, we present some benchmarks that show how our
generated JIT can handle all these features very well.

To measure the speedup we get with the JIT, we run each program three times:

\begin{enumerate}
\item By plain interpretation, without any jitting.
\item With the JIT enabled: this run includes the time spent by doing the
  compilation itself, plus the time spent by running the produced code.
\item Again with the JIT enabled, but this time the compilation has already
  been done, so we are actually measuring how good is the code we produced.
\end{enumerate}

Moreover, for each benchmark we also show the time taken by running the
equivalent program written in C\#.\footnote{The sources for both TLC and C\#
  programs are available at:

  http://codespeak.net/svn/pypy/extradoc/talk/ecoop2009/benchmarks/}

The benchmarks have been run on a machine with an Intel Pentium 4 CPU running at
3.20 GHz and 2 GB of RAM, running Microsoft Windows XP and Microsoft .NET
Framework 2.0.

\subsection{Arithmetic operations}

To benchmark arithmetic operations between integers, we wrote a simple program
that computes the factorial of a given number.  The algorithm is
straightforward, thus we are not showing the source code.  The loop contains only three operations: one
multiplication, one subtraction, and one comparison to check if we have
finished the job.

When doing plain interpretation, we need to create and destroy three temporary
objects at each iteration.  By contrast, the code generated by the JIT does
much better.  At the first iteration, the classes of the two operands of the
multiplication are promoted; then, the JIT compiler knows that both are
integers, so it can inline the code to compute the result.  Moreover, it can
\emph{virtualize} (see Section \ref{sec:virtuals}) all the temporary objects, because they never escape from
the inner loop.  The same remarks apply to the other two operations inside
the loop.

As a result, the code executed after the first iteration is close to optimal:
the intermediate values are stored as \lstinline{int} local variables, and the
multiplication, subtraction and \emph{less-than} comparison are mapped to a
single CLI opcode (\lstinline{mul}, \lstinline{sub} and \lstinline{clt},
respectively).

Similarly, we wrote a program to calculate the $n_{th}$ Fibonacci number, for
which we can do the same reasoning as above.

\begin{table}[ht]
  \begin{center}

  \begin{tabular}{l|rrrrrr}
    \multicolumn{5}{c}{\textbf{Factorial}} \\ [0.5ex]

    \textbf{$n$}          & $10$  & $10^7$           & $10^8$         & $10^9$         \\
    \hline
    \textbf{Interp}       & 0.031 & 30.984           & N/A            & N/A            \\
    \textbf{JIT}          & 0.422 &  0.453           & 0.859          & 4.844          \\
    \textbf{JIT 2}        & 0.000 &  0.047           & 0.453          & 4.641          \\
    \textbf{C\#}          & 0.000 &  0.031           & 0.359          & 3.438          \\
    \textbf{Interp/JIT 2} & N/A   & \textbf{661.000} & N/A            & N/A            \\
    \textbf{JIT 2/C\#}    & N/A   & \textbf{1.500}   & \textbf{1.261} & \textbf{1.350} \\ [3ex]


    \multicolumn{5}{c}{\textbf{Fibonacci}} \\ [0.5ex]

    \textbf{$n$}          & $10$  & $10^7$           & $10^8$         & $10^9$         \\
    \hline
    \textbf{Interp}       & 0.031 & 29.359           & 0.000          & 0.000          \\
    \textbf{JIT}          & 0.453 &  0.469           & 0.688          & 2.953          \\
    \textbf{JIT 2}        & 0.000 &  0.016           & 0.250          & 2.500          \\ 
    \textbf{C\#}          & 0.000 &  0.016           & 0.234          & 2.453          \\
    \textbf{Interp/JIT 2} & N/A   & \textbf{1879.962}& N/A            & N/A            \\
    \textbf{JIT 2/C\#}    & N/A   & \textbf{0.999}   & \textbf{1.067} & \textbf{1.019} \\
  \end{tabular}

  \end{center}
  \caption{Factorial and Fibonacci benchmarks}
  \label{tab:factorial-fibo}
\end{table}


Table \ref{tab:factorial-fibo} shows the seconds spent to calculate
the factorial and Fibonacci for various $n$.  As we can see, for small values
of $n$ the time spent running the JIT compiler is much higher than the time
spent to simply interpret the program.  This is an expected result
which, however, can be improved once we will have time
to optimize compilation and not only the generated code.

On the other, for reasonably high values of $n$ we obtain very good
results, which are valid despite the obvious overflow, since the 
same operations are performed for all experiments.
For $n$ greater than $10^7$, we did not run the interpreted program as it would have took too
much time, without adding anything to the discussion.

As we can see, the code generated by the JIT can be up to about 1800 times faster
than the non-jitted case.  Moreover, it often runs at the same speed as the
equivalent program written in C\#, being only 1.5 slower in the worst case.

The difference in speed it is probably due to both the fact that the current
CLI backend emits slightly non-optimal code and that the underyling .NET JIT
compiler is highly optimized to handle bytecode generated by C\# compilers.

As we saw in Section~\ref{sec:flexswitches-cli}, the implementation of
flexswitches on top of CLI is hard and inefficient.  However, our benchmarks
show that this inefficiency does not affect the overall performances of the
generated code.  This is because in most programs the vast majority of the
time is spent in the inner loop: the graphs are built in such a way that all
the blocks that are part of the inner loop reside in the same method, so that
all links inside are internal (and fast).


\subsection{Object-oriented features}

To measure how the JIT handles object-oriented features, we wrote a very
simple benchmark that involves attribute lookups and polymorphic method calls.
Since the TLC assembler source is long and hard to read,
figure~\ref{fig:accumulator} shows the equivalent program written in an
invented Python-like syntax.

\begin{figure}[h]
\begin{center}
\begin{lstlisting}
def main(n):
    if n < 0:
        n = -n
        obj = new(value, accumulate=count)
    else:
        obj = new(value, accumulate=add)
    obj.value = 0
    while n > 0:
        n = n - 1
        obj.accumulate(n)
    return obj.value

def count(x):
    this.value = this.value + 1

def add(x):
    this.value = this.value + x
\end{lstlisting}
\caption{The \emph{accumulator} example, written in a invented Python-like syntax}
\label{fig:accumulator}
\end{center}
\end{figure}

The two \lstinline{new} operations create an object with exactly one field
\lstinline{value} and one method \lstinline{accumulate}, whose implementation
is found in the functions \lstinline{count} and \lstinline{add}, respectively.
When calling a method, the receiver is implicity passed and can be accessed
through the special name \lstinline{this}.

The computation \emph{per se} is trivial, as it calculates either $-n$ or
$1+2...+n-1$, depending on the sign of $n$. The interesting part is the
polymorphic call to \lstinline{accumulate} inside the loop, because the interpreter has
no way to know in advance which method to call (unless it does flow analysis,
which could be feasible in this case but not in general).  The equivalent C\#
code we wrote uses two classes and a \lstinline{virtual} method call to
implement this behaviour.

As already discussed, our generated JIT does not compile the whole function at
once. Instead, it compiles and executes code chunk by chunk, waiting until it
knows enough informations to generate highly efficient code.  In particular,
at the time it emits the code for the inner loop it exactly knows the
type of \lstinline{obj}, thus it can remove the overhead of dynamic dispatch
and inline the method call.  Moreover, since \lstinline{obj} never escapes the
function, it is \emph{virtualized} and its field \lstinline{value} is stored
as a local variable.  As a result, the generated code turns out to be a simple loop
doing additions in-place.

\begin{table}[ht]
  \begin{center}

  \begin{tabular}{l|rrrrrr}
    \multicolumn{5}{c}{\textbf{Accumulator}} \\ [0.5ex]

    \textbf{$n$}          & $10$  & $10^7$           & $10^8$         & $10^9$         \\
    \hline
    \textbf{Interp}       & 0.031 & 43.063           & N/A            & N/A            \\
    \textbf{JIT}          & 0.453 &  0.516           & 0.875          & 4.188          \\
    \textbf{JIT 2}        & 0.000 &  0.047           & 0.453          & 3.672          \\
    \textbf{C\#}          & 0.000 &  0.063           & 0.563          & 5.953          \\
    \textbf{Interp/JIT 2} & N/A   & \textbf{918.765} & N/A            & N/A            \\
    \textbf{JIT 2/C\#}    & N/A   & \textbf{0.750}   & \textbf{0.806} & \textbf{0.617} \\

  \end{tabular}
  \end{center}
  \caption{Accumulator benchmark}
  \label{tab:accumulator}
\end{table}





Table \ref{tab:accumulator} show the results for the benchmark.  Again, we can
see that the speedup of the JIT over the interpreter is comparable to the
other two benchmarks.  However, the really interesting part is the comparison
with the equivalent C\# code, as the code generated by the JIT is up to 1.62 times
\textbf{faster}.

Probably, the C\# code is slower because:

\begin{itemize}
\item The object is still allocated on the heap, and thus there is an extra
  level of indirection to access the \lstinline{value} field.
\item The method call is optimized through a \emph{polymorphic inline cache}
  \cite{hoelzle_optimizing_1991}, that requires a guard check at each iteration.
\end{itemize}

Despite being only a microbenchmark, this result is very important as it proves
that our strategy of intermixing compile time and runtime can yield to better
performances than current techniques.  The result is even more impressive if
we take in account that dynamically typed languages as TLC are usually considered much
slower than the statically typed ones.
