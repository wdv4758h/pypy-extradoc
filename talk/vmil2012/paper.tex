\documentclass[10pt]{sigplanconf}

\usepackage{ifthen}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{wrapfig}
\usepackage{ulem}
\usepackage{xspace}
\usepackage{relsize}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{setspace}

\usepackage{listings}

\usepackage[T1]{fontenc}
\usepackage[scaled=0.81]{beramono}


\definecolor{commentgray}{rgb}{0.3,0.3,0.3}

\lstset{
  basicstyle=\ttfamily\footnotesize,
  language=Python,
  keywordstyle=\bfseries,
  stringstyle=\color{blue},
  commentstyle=\color{commentgray}\textit,
  fancyvrb=true,
  showstringspaces=false,
  %keywords={def,while,if,elif,return,class,get,set,new,guard_class}
  numberstyle = \tiny,
  numbersep = -20pt,
}

\newboolean{showcomments}
\setboolean{showcomments}{true}
\ifthenelse{\boolean{showcomments}}
  {\newcommand{\nb}[2]{
    \fbox{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
   }
   \newcommand{\version}{\emph{\scriptsize$-$Id: main.tex 19055 2008-06-05 11:20:31Z cfbolz $-$}}
  }
  {\newcommand{\nb}[2]{}
   \newcommand{\version}{}
  }

\newcommand\cfbolz[1]{\nb{CFB}{#1}}
\newcommand\toon[1]{\nb{TOON}{#1}}
\newcommand\anto[1]{\nb{ANTO}{#1}}
\newcommand\arigo[1]{\nb{AR}{#1}}
\newcommand\fijal[1]{\nb{FIJAL}{#1}}
\newcommand\pedronis[1]{\nb{PEDRONIS}{#1}}
\newcommand\bivab[1]{\nb{DAVID}{#1}}
\newcommand{\commentout}[1]{}

\newcommand{\noop}{}


\newcommand\ie{i.e.,\xspace}
\newcommand\eg{e.g.,\xspace}

\normalem

\let\oldcite=\cite

\renewcommand\cite[1]{\ifthenelse{\equal{#1}{XXX}}{[citation~needed]}{\oldcite{#1}}}

\definecolor{gray}{rgb}{0.5,0.5,0.5}

\begin{document}

\title{Efficiently Handling Guards in the low level design of RPython's tracing JIT}

\authorinfo{Carl Friedrich Bolz$^a$ \and David Schneider$^{a}$}
           {$^a$Heinrich-Heine-Universität Düsseldorf, STUPS Group, Germany
           }
           {XXX emails}

\conferenceinfo{VMIL'12}{}
\CopyrightYear{2012}
\crdata{}

\maketitle

\category{D.3.4}{Programming Languages}{Processors}[code generation,
incremental compilers, interpreters, run-time environments]

\terms
Languages, Performance, Experimentation

\keywords{XXX}

\begin{abstract}

\end{abstract}


%___________________________________________________________________________
\section{Introduction}

In this paper we describe and analyze how deoptimization works in the context
of tracing just-in-time compilers. What instructions are used in the
intermediate and low-level representation of the JIT instructions and how these
are implemented.

Although there are several publications about tracing jut-in-time compilers, to
our knowledge, there are none that describe the use and implementaiton of
guards in this context. With the following contributions we aim to sched some
light (to much?) on this topic.
The contributions of this paper are:
\begin{itemize}
 \item
\end{itemize}

The paper is structured as follows:

\section{Background}
\label{sec:Background}

\subsection{RPython and the PyPy Project}
\label{sub:pypy}


The RPython language and the PyPy Project were started in 2002 with the goal of
creating a Python interpreter written in a high level language, allowing easy
language experimentation and extension. PyPy is now a fully compatible
alternative implementation of the Python language\bivab{mention speed}. The
Implementation takes advantage of the language features provided by RPython
such as the provided tracing just-in-time compiler described below.

RPython, the language and the toolset originally developed to implement the
Python interpreter have developed into a general environment for experimenting
and developing fast and maintainable dynamic language implementations.
\bivab{Mention the different language impls}

RPython is built of two components, the language and the translation toolchain
used to transform RPython programs to executable units.  The RPython language
is a statically typed object oriented high level language. The language provides
several features such as automatic memory management (aka. Garbage Collection)
and just-in-time compilation. When writing an interpreter using RPython the
programmer only has to write the interpreter for the language she is
implementing.  The second RPython component, the translation toolchain, is used
to transform the program to a low level representations suited to be compiled
and run on one of the different supported target platforms/architectures such
as C, .NET and Java. During the transformation process
different low level aspects suited for the target environment are automatically
added to program such as (if needed) a garbage collector and with some hints
provided by the author a just-in-time compiler.



\subsection{PyPy's Meta-Tracing JIT Compilers}
\label{sub:tracing}

 * Tracing JITs
 * JIT Compiler
   * describe the tracing jit stuff in pypy
   * reference tracing the meta level paper for a high level description of what the JIT does
   * JIT Architecture
   * Explain the aspects of tracing and optimization

%___________________________________________________________________________


\section{Resume Data}
\label{sec:Resume Data}

* High level handling of resumedata

- traces follow the execution path during tracing, other path not compiled at first
- points of possible divergence from that path are guards
- since path can later diverge, at the guards it must be possible to re-build interpreter state in the form of interpreter stack frames
- tracing does inlining, therefore a guard must contain information to build a whole stack of frames
- optimization rewrites traces, including removal of guards

- frames consist of a PC and local variables
- rebuild frame by taking local SSA variables in the trace and mapping them to variables in the frame

two forces:
- there are lots of guards, therefore the information must be stored in a compact way in the end
- tracing must be fast

compression approaches:
- use fact that outer frames don't change in the part of the trace that is in the inner frame
- compact bit-representation for constants/ssa vars

interaction with optimization
- guard coalescing
- virtuals
  - most virtuals not changed between guards


   * tracing and attaching bridges and throwing away resume data
   * compiling bridges
\bivab{mention that the failargs also go into the bridge}
% section Resume Data (end)

\section{Guards in the Backend}
\label{sec:Guards in the Backend}

Code generation consists of two passes over the lists of instructions, a
backwards pass to calculate live ranges of IR-level variables and a forward one
to emit the instructions. During the forward pass IR-level variables are
assigned to registers and stack locations by the register allocator according
to the requirements of the to be emitted instructions. Eviction/spilling is
performed based on the live range information collected in the first pass. Each
IR instruction is transformed into one or more machine level instructions that
implement the required semantics, operations withouth side effects whose result
is not used are not emitted. Guards instructions are transformed into fast
checks at the machine code level that verify the corresponding condition.  In
cases the value being checked by the guard is not used anywhere else the guard
and the operation producing the value can merged, reducing even more the
overhead of the guard. \bivab{example for this}

Each guard in the IR has attached to it a list of the IR-variables required to
rebuild the execution state in case the trace is left through the side-exit
corresponding to the guard. When a guard is compiled, additionally to the
condition check two things are generated/compiled. First a special
data structure is created that encodes the information provided by the register
allocator about where the values corresponding to each IR-variable required by
the guard will be stored when execution reaches the code emitted for the
corresponding guard. \bivab{go into more detail here?!} This encoding needs to
be as compact as possible to maintain an acceptable memory profile.

\bivab{example goes here}

Second a trampoline method stub is generated. Guards are usually implemented as
a conditional jump to this stub, jumping to in case the guard condition check
does not hold and a side-exit should be taken. This stub loads the pointer to
the encoding of the locations mentioned above, preserves the execution state
(stack and registers) and then jumps to generic bail-out handler that is used
to leave the compiled trace if case of a guard failure.

Using the encoded location information the bail-out handler reads from the
saved execution state the values that the IR-variables had  at the time of the
guard failure and stores them in a location that can be read by the fronted.
After saving the information the control is passed to the frontend signaling
which guard failed so the frontend can read the information passed and restore
the state corresponding to the point in the program.

As in previous sections the underlying idea for the design of guards is to have
a fast on trace profile and a potentially slow one in the bail-out case where
the execution takes one of the side exits due to a guard failure. At the same
time the data stored in the backend needed to rebuild the state should be be
as compact as possible to reduce the memory overhead produced by the large
number of guards\bivab{back this}.

As explained in previous sections, when a specific guard has failed often enogh
a new trace, refered to as bridge, starting from this guard is recorded and
compiled. When compiling bridges the goal is that future failures of the guards
that led to the compilation of the bridge should execute the bridge without
additional overhead, in particular it the failure of the guard should not lead
to leaving the compiled code prior to execution the code of the bridge.

The process of compiling a bridge is very similar to compiling a loop,
instructions and guards are processed in the same way as described above. The
main difference is the setup phase, when compiling a trace we start with a lean
slate. The compilation of a bridge is starts from a state (register and stack
bindings) that corresponds to the state during the compilation of the original
guard. To restore the state needed compile the bridge we use the encoded
representation created for the guard to rebuild the bindings from IR-variables
to stack locations and registers used in the register allocator.  With this
reconstruction all bindings are restored to the state as they were in the
original loop up to the guard.

Once the bridge has been compiled the trampoline method stub is redirected to
the code of the bridge. In future if the guard fails again it jumps to the code
compiled for the bridge instead of bailing out. Once the guard has been
compiled and attached to the loop the guard becomes just a point where
control-flow can split. The loop after the guard and the bridge are just
condional paths.

%* Low level handling of guards
%   * Fast guard checks v/s memory usage
%   * memory efficient encoding of low level resume data
%   * fast checks for guard conditions
%   * slow bail out
%
% section Guards in the Backend (end)

%___________________________________________________________________________


\section{Evaluation}
\label{sec:evaluation}

* Evaluation
   * Measure guard memory consumption and machine code size
   * Extrapolate memory consumption for guard other guard encodings
      * compare to naive variant
   * Measure how many guards survive optimization
   * Measure the of guards and how many of these ever fail

\section{Related Work}


\section{Conclusion}


\section*{Acknowledgements}

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}
