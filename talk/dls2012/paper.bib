
@inproceedings{deutsch_efficient_1984,
	address = {Salt Lake City, Utah},
	title = {Efficient implementation of the {S}malltalk-80 system},
	isbn = {0-89791-125-3},
	url = {http://portal.acm.org/citation.cfm?id=800017.800542},
	doi = {10.1145/800017.800542},
	abstract = {The Smalltalk-80* programming language includes dynamic storage allocation, full upward funargs, and universally polymorphic procedures; the Smalltalk-80 programming system features interactive execution with incremental compilation, and implementation portability. These features of modern programming systems are among the most difficult to implement efficiently, even individually. A new implementation of the Smalltalk-80 system, hosted on a small microprocessor-based computer, achieves high performance while retaining complete (object code) compatibility with existing implementations. This paper discusses the most significant optimization techniques developed over the course of the project, many of which are applicable to other languages. The key idea is to represent certain runtime state (both code and data) in more than one form, and to convert between forms when needed.},
	booktitle = {{POPL}},
	publisher = {{ACM}},
	author = {Deutsch, L. Peter and Schiffman, Allan M.},
	year = {1984}
},

@inproceedings{bolz_towards_2010,
	address = {Hagenberg, Austria},
	title = {Towards a Jitting {VM} for {P}rolog execution},
	isbn = {978-1-4503-0132-9},
	url = {http://portal.acm.org/citation.cfm?id=1836102},
	doi = {10.1145/1836089.1836102},
	abstract = {Most Prolog implementations are implemented in low-level languages such as C and are based on a variation of the {WAM} instruction set, which enhances their performance but makes them hard to write. In addition, many of the more dynamic features of Prolog (like assert), despite their popularity, are not well supported. We present a high-level continuation-based Prolog interpreter based on the {PyPy} project. The {PyPy} project makes it possible to easily and efficiently implement dynamic languages. It provides tools that automatically generate a just-in-time compiler for a given interpreter of the target language, by using partial evaluation techniques. The resulting Prolog implementation is surprisingly efficient: it clearly outperforms existing interpreters of Prolog in high-level languages such as Java. Moreover, on some benchmarks, our system outperforms state-of-the-art {WAM-based} Prolog implementations. Our paper aims to show that declarative languages such as Prolog can indeed benefit from having a just-in-time compiler and that {PyPy} can form the basis for implementing programming languages other than Python.},
	booktitle = {{PPDP}},
	publisher = {{ACM}},
	author = {Bolz, Carl Friedrich and Leuschel, Michael and Schneider, David},
	year = {2010},
	keywords = {interpreters, jit, logic programming, partial evaluation}
},

@inproceedings{bebenita_spur:_2010,
	address = {{Reno/Tahoe}, Nevada, {USA}},
	title = {{SPUR:} a trace-based {JIT} compiler for {CIL}},
	isbn = {978-1-4503-0203-6},
	shorttitle = {{SPUR}},
	url = {http://portal.acm.org/citation.cfm?id=1869459.1869517&coll=GUIDE&dl=GUIDE&type=series&idx=SERIES318&part=series&WantType=Proceedings&title=OOPSLA%2FSPLASH&CFID=106280261&CFTOKEN=29377718},
	doi = {10.1145/1869459.1869517},
	abstract = {Tracing just-in-time compilers {(TJITs)} determine frequently executed traces (hot paths and loops) in running programs and focus their optimization effort by emitting optimized machine code specialized to these traces. Prior work has established this strategy to be especially beneficial for dynamic languages such as {JavaScript}, where the {TJIT} interfaces with the interpreter and produces machine code from the {JavaScript} trace.},
	booktitle = {{OOPSLA}},
	publisher = {{ACM}},
	author = {Bebenita, Michael and Brandner, Florian and Fahndrich, Manuel and Logozzo, Francesco and Schulte, Wolfram and Tillmann, Nikolai and Venter, Herman},
	year = {2010},
	keywords = {cil, dynamic compilation, javascript, just-in-time, tracing}
},

@inproceedings{gal_trace-based_2009,
	address = {New York, New York},
	series = {{PLDI} '09},
	title = {Trace-based just-in-time type specialization for dynamic languages},
	isbn = {978-1-60558-392-1},
	location = {Dublin, Ireland},
	doi = {10.1145/1542476.1542528},
	abstract = {Dynamic languages such as {JavaScript} are more difficult to compile than statically typed ones. Since no concrete type information is available, traditional compilers need to emit generic code that can handle all possible type combinations at runtime. We present an alternative compilation technique for dynamically-typed languages that identifies frequently executed loop traces at run-time and then generates machine code on the fly that is specialized for the actual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural type specialization, and an elegant and efficient way of incrementally compiling lazily discovered alternative paths through nested loops. We have implemented a dynamic compiler for {JavaScript} based on our technique and we have measured speedups of 10x and more for certain benchmark programs.},
	booktitle = {{PLDI}},
	publisher = {{ACM}},
	author = {Gal, Andreas and Eich, Brendan and Shaver, Mike and Anderson, David and Mandelin, David and Haghighat, Mohammad R and Kaplan, Blake and Hoare, Graydon and Zbarsky, Boris and Orendorff, Jason and Ruderman, Jesse and Smith, Edwin W and Reitmaier, Rick and Bebenita, Michael and Chang, Mason and Franz, Michael},
	year = {2009},
	note = {{ACM} {ID:} 1542528},
	keywords = {code generation, design, dynamically typed languages, experimentation, incremental compilers, languages, measurement, performance, run-time environments, trace-based compilation}
},

@inproceedings{kotzmann_escape_2005,
	address = {New York, {NY}, {USA}},
	series = {{VEE} '05},
	title = {Escape analysis in the context of dynamic compilation and deoptimization},
	isbn = {1-59593-047-7},
	location = {Chicago, {IL}, {USA}},
	doi = {10.1145/1064979.1064996},
	abstract = {In object-oriented programming languages, an object is said to escape the method or thread in which it was created if it can also be accessed by other methods or threads. Knowing which objects do not escape allows a compiler to perform aggressive {optimizations.This} paper presents a new intraprocedural and interprocedural algorithm for escape analysis in the context of dynamic compilation where the compiler has to cope with dynamic class loading and deoptimization. It was implemented for Sun Microsystems' Java {HotSpot™} client compiler and operates on an intermediate representation in {SSA} form. We introduce equi-escape sets for the efficient propagation of escape information between related objects. The analysis is used for scalar replacement of fields and synchronization removal, as well as for stack allocation of objects and fixed-sized arrays. The results of the interprocedural analysis support the compiler in inlining decisions and allow actual parameters to be allocated on the caller {stack.Under} certain circumstances, the Java {HotSpot™} {VM} is forced to stop executing a method's machine code and transfer control to the interpreter. This is called deoptimization. Since the interpreter does not know about the scalar replacement and synchronization removal performed by the compiler, the deoptimization framework was extended to reallocate and relock objects on demand.},
	booktitle = {Proceedings of the 1st {ACM/USENIX} international conference on Virtual execution environments},
	publisher = {{ACM}},
	author = {Kotzmann, Thomas and Mössenböck, Hanspeter},
	year = {2005},
	note = {{ACM} {ID:} 1064996},
	keywords = {algorithms, allocation/deallocation strategies, deoptimization},
	pages = {111–120}
},

@inproceedings{bolz_towards_2009,
	title = {Towards Just-In-Time Partial Evaluation of Prolog},
	doi = {10.1007/978-3-642-12592-8_12},
	booktitle = {Logic Program Synthesis and Transformation},
	author = {Bolz, Carl Friedrich and Leuschel, Michael and Rigo, Armin},
	year = {2009},
	pages = {158–172}
},

@inproceedings{bolz_allocation_2011,
	address = {Austin, Texas, {USA}},
	title = {Allocation removal by partial evaluation in a tracing {JIT}},
	abstract = {The performance of many dynamic language implementations suffers from high allocation rates and runtime type checks. This makes dynamic languages less applicable to purely algorithmic problems, despite their growing popularity. In this paper we present a simple compiler optimization based on online partial evaluation to remove object allocations and runtime type checks in the context of a tracing {JIT.} We evaluate the optimization using a Python {VM} and find that it gives good results for all our (real-life) benchmarks.},
	booktitle = {{PEPM}},
	author = {Bolz, Carl Friedrich and Cuni, Antonio and Fijałkowski, Maciej and Leuschel, Michael and Pedroni, Samuele and Rigo, Armin},
	year = {2011},
	keywords = {code generation, experimentation, interpreters, languages, optimization, partial evaluation, performance, run-time environments, tracing jit}
},

@article{hiniker_improving_2005,
	series = {{MICRO} 38},
	title = {Improving Region Selection in Dynamic Optimization Systems},
	location = {Barcelona, Spain},
	url = {http://dx.doi.org/10.1109/MICRO.2005.22},
	doi = {http://dx.doi.org/10.1109/MICRO.2005.22},
	abstract = {The performance of a dynamic optimization system depends heavily on the code it selects to optimize. Many current systems follow the design of {HP} Dynamo and select a single interprocedural path, or trace, as the unit of code optimization and code caching. Though this approach to region selection has worked well in practice, we show that it is possible to adapt this basic approach to produce regions with greater locality, less needless code duplication, and fewer profiling counters. In particular, we propose two new region-selection algorithms and evaluate them against Dynamo¿s selection mechanism, Next-Executing Tail {(NET).} Our first algorithm, Last-Executed Iteration {(LEI)}, identifies cyclic paths of execution better than {NET}, improving locality of execution while reducing the size of the code cache. Our second algorithm allows overlapping traces of similar execution frequency to be combined into a single large region. This second technique can be applied to both {NET} and {LEI}, and we find that it significantly improves metrics of locality and memory overhead for each.},
	journal = {Proceedings of the 38th annual {IEEE/ACM} International Symposium on Microarchitecture},
	author = {Hiniker, David and Hazelwood, Kim and Smith, Michael D},
	year = {2005},
	note = {{ACM} {ID:} 1100546},
	keywords = {microprocessors and microcomputers, optimization, performance},
	pages = {141–154}
},

@book{muchnick_advanced_1997,
	title = {Advanced Compiler Design and Implementation},
	isbn = {9781558603202},
	publisher = {Morgan Kaufmann},
	author = {Muchnick, Steven S. and Muchnick},
	month = sep,
	year = {1997}
},

@misc{pall_luajit_2009,
	title = {{LuaJIT} 2.0 intellectual property disclosure and research opportunities},
	url = {http://lua-users.org/lists/lua-l/2009-11/msg00089.html},
	author = {Pall, Mike},
	month = nov,
	year = {2009}
},

@inproceedings{bolz_runtime_2011,
	address = {Lancaster, {UK}},
	title = {Runtime Feedback in a Meta-Tracing {JIT} for Efficient Dynamic Languages},
	abstract = {Meta-tracing {JIT} compilers can be applied to a variety of different languages without explicitly encoding language semantics into the compiler. So far, they lacked a way to give the language implementor control over runtime feedback. This restricted their performance. In this paper we describe the mechanisms in {PyPy’s} meta-tracing {JIT} that can be used to control runtime feedback in language-specific ways. These mechanisms are flexible enough to express classical {VM} techniques such as maps and runtime type feedback.},
	booktitle = {{ICOOOLPS}},
	publisher = {{ACM}},
	author = {Bolz, Carl Friedrich and Cuni, Antonio and Fijałkowski, Maciej and Leuschel, Michael and Rigo, Armin and Pedroni, Samuele},
	year = {2011}
},

@inproceedings{chang_tracing_2009,
	address = {Washington, {DC}},
	title = {Tracing for {W}eb 3.0: Trace Compilation for the Next Generation Web Applications},
	isbn = {978-1-60558-375-4},
	shorttitle = {Tracing for web 3.0},
	url = {http://portal.acm.org/citation.cfm?id=1508293.1508304},
	doi = {10.1145/1508293.1508304},
	abstract = {Today's web applications are pushing the limits of modern web browsers. The emergence of the browser as the platform of choice for rich client-side applications has shifted the use of in-browser {JavaScript} from small scripting programs to large computationally intensive application logic. For many web applications, {JavaScript} performance has become one of the bottlenecks preventing the development of even more interactive client side applications. While traditional just-in-time compilation is successful for statically typed virtual machine based languages like Java, compiling {JavaScript} turns out to be a challenging task. Many {JavaScript} programs and scripts are short-lived, and users expect a responsive browser during page loading. This leaves little time for compilation of {JavaScript} to generate machine code.},
	booktitle = {{VEE}},
	publisher = {{ACM}},
	author = {Chang, Mason and Smith, Edwin and Reitmaier, Rick and Bebenita, Michael and Gal, Andreas and Wimmer, Christian and Eich, Brendan and Franz, Michael},
	year = {2009},
	keywords = {dynamically typed languages, forth, tamarin, trace trees, tracing, type specialization},
	pages = {71--80}
},

@article{diwan_type-based_1998,
	title = {Type-based alias analysis},
	volume = {33},
	issn = {0362-1340},
	url = {http://doi.acm.org/10.1145/277652.277670},
	doi = {10.1145/277652.277670},
	abstract = {This paper evaluates three alias analyses based on programming language types. The first analysis uses type compatibility to determine aliases. The second extends the first by using additional high-level information such as field names. The third extends the second with a flow-insensitive analysis. Although other researchers suggests using types to disambiguate memory references, none evaluates its effectiveness. We perform both static and dynamic evaluations of type-based alias analyses for Modula-3, a statically-typed type-safe language. The static analysis reveals that type compatibility alone yields a very imprecise alias analysis, but the other two analyses significantly improve alias precision. We use redundant load elimination {(RLE)} to demonstrate the effectiveness of the three alias algorithms in terms of the opportunities for optimization, the impact on simulated execution times, and to compute an upper bound on what a perfect alias analysis would yield. We show modest dynamic improvements for {(RLE)}, and more surprisingly, that on average our alias analysis is within 2.5\% of a perfect alias analysis with respect to {RLE} on 8 Modula-3 programs. These results illustrate that to explore thoroughly the effectiveness of alias analyses, researchers need static, dynamic, and upper-bound analysis. In addition, we show that for type-safe languages like Modula-3 and Java, a fast and simple alias analysis may be sufficient for many applications.},
	number = {5},
	journal = {{SIGPLAN} Not.},
	author = {Diwan, Amer and {McKinley}, Kathryn S. and Moss, J. Eliot B.},
	month = may,
	year = {1998},
	pages = {106–117}
},

@inproceedings{cytron_code_1986,
	address = {New York, {NY}, {USA}},
	series = {{POPL} '86},
	title = {Code motion of control structures in high-level languages},
	url = {http://doi.acm.org/10.1145/512644.512651},
	doi = {10.1145/512644.512651},
	abstract = {One trend among programmers is the increased use of abstractions. Through encapsulation techniques, abstractions extend the repertory of data structures and their concomitant operations that are processed directly by a compiler. For example, a compiler might not offer sets or set operations in its base language, but abstractions allow a programmer to define sets in terms of constructs already recognized by the compiler. In particular, abstractions can allow new constructs to be defined in terms of other abstractions. Although significant power is gained through the use of layered abstractions, object code quality suffers as increasingly less of a program's data structures and operations are exposed to the optimization phase of a compiler. Multiple references to abstractions are also inefficient, since the interaction between abstractions is often complex yet hidden from a compiler. Abstractions are most flexible when they are cast in general terms; a specific invocation is then tailored by the abstraction to obtain the appropriate code. A sequence of references to such abstractions can be inefficient due to functional redundancy that cannot be detected at compile-time. By integrating the references, the offending segments of code can be moved to a more advantageous position. Although procedure integration materializes abstracted constructs, the abstractions can still be ineligible for optimization using current techniques; in particular, abstractions often involve loops and conditional branches that can obscure code that would otherwise be eligible for code motion.},
	booktitle = {Proceedings of the 13th {ACM} {SIGACT-SIGPLAN} symposium on Principles of programming languages},
	publisher = {{ACM}},
	author = {Cytron, Ron and Lowry, Andy and Zadeck, F. Kenneth},
	year = {1986},
	pages = {70–85}
},

@article{knoop_lazy_1992,
	title = {Lazy code motion},
	volume = {27},
	issn = {0362-1340},
	url = {http://doi.acm.org/10.1145/143103.143136},
	doi = {10.1145/143103.143136},
	abstract = {We present a bit-vector algorithm for the optimal and economical placement of computations within flow graphs, which is as efficient as standard uni-directional analyses. The point of our algorithm is the decomposition of the bi-directional structure of the known placement algorithms into a sequence of a backward and a forward analysis, which directly implies the efficiency result. Moreover, the new compositional structure opens the algorithm for modification: two further uni-directional analysis components exclude any unnecessary code motion. This laziness of our algorithm minimizes the register pressure, which has drastic effects on the run-time behaviour of the optimized programs in practice, where an economical use of registers is essential.},
	number = {7},
	journal = {{SIGPLAN} Not.},
	author = {Knoop, Jens and Rüthing, Oliver and Steffen, Bernhard},
	month = jul,
	year = {1992},
	pages = {224–234}
},

@incollection{allen_catalogue_1971,
	title = {A Catalogue of Optimizing Transformations},
	booktitle = {Design and Optimization of Compilers},
	publisher = {Prentice-Hall},
	author = {Allen, Frances and Cocke, John},
	editor = {Rustin, Randall},
	year = {1971},
	pages = {1--30}
},

@inproceedings{chow_new_1997,
	address = {New York, {NY}, {USA}},
	series = {{PLDI} '97},
	title = {A new algorithm for partial redundancy elimination based on {SSA} form},
	isbn = {0-89791-907-6},
	url = {http://doi.acm.org/10.1145/258915.258940},
	doi = {10.1145/258915.258940},
	abstract = {A new algorithm, {SSAPRE}, for performing partial redundancy elimination based entirely on {SSA} form is presented. It achieves optimal code motion similar to lazy code motion {[KRS94a}, {DS93]}, but is formulated independently and does not involve iterative data flow analysis and bit vectors in its solution. It not only exhibits the characteristics common to other sparse approaches, but also inherits the advantages shared by other {SSA-based} optimization techniques. {SSAPRE} also maintains its output in the same {SSA} form as its input. In describing the algorithm, we state theorems with proofs giving our claims about {SSAPRE.} We also give additional description about our practical implementation of {SSAPRE}, and analyze and compare its performance with a bit-vector-based implementation of {PRE.} We conclude with some discussion of the implications of this work.},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 1997 conference on Programming language design and implementation},
	publisher = {{ACM}},
	author = {Chow, Fred and Chan, Sun and Kennedy, Robert and Liu, Shin-Ming and Lo, Raymond and Tu, Peng},
	year = {1997},
	pages = {273–286}
},

@article{morel_global_1979,
	title = {Global optimization by suppression of partial redundancies},
	volume = {22},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/359060.359069},
	doi = {10.1145/359060.359069},
	abstract = {The elimination of redundant computations and the moving of invariant computations out of loops are often done separately, with invariants moved outward loop by loop. We propose to do both at once and to move each expression directly to the entrance of the outermost loop in which it is invariant. This is done by solving a more general problem, i.e. the elimination of computations performed twice on a given execution path. Such computations are termed partially redundant. Moreover, the algorithm does not require any graphical information or restrictions on the shape of the program graph. Testing this algorithm has shown that its execution cost is nearly linear with the size of the program, and that it leads to a smaller optimizer that requires less execution time.},
	number = {2},
	journal = {Commun. {ACM}},
	author = {Morel, E. and Renvoise, C.},
	month = feb,
	year = {1979},
	keywords = {Boolean systems, compilation, compiler, data flow analysis, invariant computation elimination, optimization, optimizer, partial redundancy, redundancy elimination},
	pages = {96–103}
},

@article{dhamdhere_practical_1991,
	title = {Practical adaption of the global optimization algorithm of {M}orel and {R}envoise},
	volume = {13},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/103135.214520},
	doi = {10.1145/103135.214520},
	number = {2},
	journal = {{ACM} Trans. Program. Lang. Syst.},
	author = {Dhamdhere, D. M.},
	month = apr,
	year = {1991},
	pages = {291–294}
},

@phdthesis{chow_portable_1984,
	address = {Stanford, {CA}, {USA}},
	title = {A portable machine-independent global optimizer–design and measurements},
	school = {Stanford University},
	author = {Chow, Frederick Chi-Tak},
	year = {1984},
	note = {{AAI8408268}}
},

@inproceedings{ancona_rpython:_2007,
	address = {Montreal, Quebec, Canada},
	title = {{RPython:} a step towards reconciling dynamically and statically typed {OO} languages},
	isbn = {978-1-59593-868-8},
	shorttitle = {{RPython}},
	url = {http://portal.acm.org/citation.cfm?id=1297091},
	doi = {10.1145/1297081.1297091},
	abstract = {Although the C-based interpreter of Python is reasonably fast, implementations on the {CLI} or the {JVM} platforms offers some advantages in terms of robustness and interoperability. Unfortunately, because the {CLI} and {JVM} are primarily designed to execute statically typed, object-oriented languages, most dynamic language implementations cannot use the native bytecodes for common operations like method calls and exception handling; as a result, they are not able to take full advantage of the power offered by the {CLI} and {JVM.}},
	booktitle = {{DLS}},
	publisher = {{ACM}},
	author = {Ancona, Davide and Ancona, Massimo and Cuni, Antonio and Matsakis, Nicholas D.},
	year = {2007},
	keywords = {{JVM}, .net, Python}
},

@article{futamura_partial_1999,
	title = {Partial Evaluation of Computation Process - An Approach to a Compiler-Compiler},
	volume = {12},
	url = {http://citeseer.ist.psu.edu/futamura99partial.html},
	number = {4},
	journal = {Higher-Order and Symbolic Computation},
	author = {Futamura, Yoshihiko},
	year = {1999},
	keywords = {Futamura},
	pages = {381--391}
},

@book{jones_partial_1993,
	title = {Partial evaluation and automatic program generation},
	isbn = {0-13-020249-5},
	url = {http://portal.acm.org/citation.cfm?id=153676},
	abstract = {This book is out of print. For copies, Please refer to the following online page},
	publisher = {Prentice-Hall},
	author = {Jones, Neil D. and Gomard, Carsten K. and Sestoft, Peter},
	year = {1993}
},

@inproceedings{rigo_pypys_2006,
	address = {Portland, Oregon, {USA}},
	title = {{PyPy's} approach to virtual machine construction},
	isbn = {1-59593-491-X},
	url = {http://portal.acm.org/citation.cfm?id=1176753},
	doi = {10.1145/1176617.1176753},
	abstract = {The {PyPy} project seeks to prove both on a research and a practical level the feasibility of constructing a virtual machine {(VM)} for a dynamic language in a dynamic language - in this case, Python. The aim is to translate (i.e. compile) the {VM} to arbitrary target environments, ranging in level from {C/Posix} to {Smalltalk/Squeak} via Java and {CLI/.NET}, while still being of reasonable efficiency within these {environments.A} key tool to achieve this goal is the systematic reuse of the Python language as a system programming language at various levels of our architecture and translation process. For each level, we design a corresponding type system and apply a generic type inference engine - for example, the garbage collector is written in a style that manipulates simulated pointer and address objects, and when translated to C these operations become C-level pointer and address instructions.},
	booktitle = {{DLS}},
	publisher = {{ACM}},
	author = {Rigo, Armin and Pedroni, Samuele},
	year = {2006},
	keywords = {metacircularity, Python, retargettable code generation, type inference, {VM}}
},

@article{georges_statistically_2007,
	title = {Statistically rigorous {J}ava performance evaluation},
	volume = {42},
	url = {http://portal.acm.org/citation.cfm?id=1297105.1297033},
	doi = {10.1145/1297105.1297033},
	abstract = {Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time {(JIT)} compilation and optimization in the virtual machine {(VM)} driven by timer-based method sampling, thread scheduling, garbage collection, and various.},
	number = {10},
	journal = {{SIGPLAN} Notices},
	author = {Georges, Andy and Buytaert, Dries and Eeckhout, Lieven},
	year = {2007},
	keywords = {benchmarking, data analysis, methodology, statistics},
	pages = {57--76}
},

@inproceedings{bolz_tracing_2009,
	address = {Genova, Italy},
	title = {Tracing the meta-level: {PyPy's} tracing {JIT} compiler},
	isbn = {978-1-60558-541-3},
	shorttitle = {Tracing the meta-level},
	url = {http://portal.acm.org/citation.cfm?id=1565827},
	doi = {10.1145/1565824.1565827},
	abstract = {We attempt to apply the technique of Tracing {JIT} Compilers in the context of the {PyPy} project, i.e., to programs that are interpreters for some dynamic languages, including Python. Tracing {JIT} compilers can greatly speed up programs that spend most of their time in loops in which they take similar code paths. However, applying an unmodified tracing {JIT} to a program that is itself a bytecode interpreter results in very limited or no speedup. In this paper we show how to guide tracing {JIT} compilers to greatly improve the speed of bytecode interpreters. One crucial point is to unroll the bytecode dispatch loop, based on two kinds of hints provided by the implementer of the bytecode interpreter. We evaluate our technique by applying it to two {PyPy} interpreters: one is a small example, and the other one is the full Python interpreter.},
	booktitle = {{ICOOOLPS}},
	publisher = {{ACM}},
	author = {Bolz, Carl Friedrich and Cuni, Antonio and Fijałkowski, Maciej and Rigo, Armin},
	year = {2009},
	pages = {18--25}
},

@article{bala_dynamo:_2000,
	title = {Dynamo: a transparent dynamic optimization system},
	volume = {35},
	shorttitle = {Dynamo},
	url = {http://citeseer.ist.psu.edu/bala00dynamo.html},
	number = {5},
	journal = {{ACM} {SIGPLAN} Notices},
	author = {Bala, Vasanth and Duesterwald, Evelyn and Banerjia, Sanjeev},
	year = {2000},
	keywords = {toread},
	pages = {1--12}
},

@techreport{gal_incremental_2006,
	title = {Incremental Dynamic Code Generation with Trace Trees},
	abstract = {The unit of compilation for traditional just-in-time compilers is the method. We have explored trace-based compilation, in which the unit of compilation is a loop, potentially spanning multiple methods and even library code. Using a new intermediate representation that is discovered and updated lazily on-demand while the program is being executed, our compiler generates code that is competitive with traditional dynamic compilers, but that uses only a fraction of the compile time and memory footprint.},
	number = {{ICS-TR-06-16}},
	institution = {Donald Bren School of Information and Computer Science, University of California, Irvine},
	author = {Gal, Andreas and Franz, Michael},
	month = nov,
	year = {2006},
	pages = {11}
},

@inproceedings{gal_hotpathvm:_2006,
	address = {Ottawa, Ontario, Canada},
	title = {{HotpathVM:} an effective {JIT} compiler for resource-constrained devices},
	isbn = {1-59593-332-6},
	shorttitle = {{HotpathVM}},
	url = {http://portal.acm.org/citation.cfm?doid=1134760.1134780},
	doi = {10.1145/1134760.1134780},
	abstract = {We present a just-in-time compiler for a Java {VM} that is small enough to fit on resource-constrained devices, yet is surprisingly effective. Our system dynamically identifies traces of frequently executed bytecode instructions (which may span several basic blocks across several methods) and compiles them via Static Single Assignment {(SSA)} construction. Our novel use of {SSA} form in this context allows to hoist instructions across trace side-exits without necessitating expensive compensation code in off-trace paths. The overall memory consumption (code and data) of our system is only 150 {kBytes}, yet benchmarks show a speedup that in some cases rivals heavy-weight just-in-time compilers.},
	booktitle = {{VEE}},
	publisher = {{ACM}},
	author = {Gal, Andreas and Probst, Christian W. and Franz, Michael},
	year = {2006},
	keywords = {dynamic compilation, embedded, software trace scheduling, {SSA}, {VM}}
},

@inproceedings{wolczko_towards_1999,
	title = {Towards a Universal Implementation Substrate for Object-Oriented Languages},
	abstract = {Self is a minimalist object-oriented language with a sophisticated implementation that utilizes adaptive optimization. We have built implementations of Smalltalk and Java by translation to Self. These implementations were much easier to construct in Self than by conventional means, and perform surprisingly well (competitively with conventional, commercial implementations). This leads us to believe that a Self-like system may form the basis of a universal substrate for implementation of object-oriented languages.},
	booktitle = {{OOPSLA} workshop on Simplicity, Performance, and Portability in Virtual Machine Design},
	author = {Wolczko, Mario and Agesen, Ole and {{David} Ungar}},
	year = {1999},
	keywords = {fixme}
},

@inproceedings{holzle_optimizing_1994,
	address = {Orlando, Florida, United States},
	title = {Optimizing dynamically-dispatched calls with run-time type feedback},
	isbn = {0-89791-662-X},
	url = {http://portal.acm.org/citation.cfm?id=178243.178478},
	doi = {10.1145/178243.178478},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {{PLDI}},
	publisher = {{ACM}},
	author = {Hölzle, Urs and Ungar, David},
	year = {1994},
	keywords = {{JIT}, polymorphic inline cache, self, type-feedback},
	pages = {326--336}
},

@inproceedings{yermolovich_optimization_2009,
	address = {Orlando, Florida, {USA}},
	title = {Optimization of dynamic languages using hierarchical layering of virtual machines},
	isbn = {978-1-60558-769-1},
	url = {http://portal.acm.org/citation.cfm?id=1640134.1640147},
	doi = {10.1145/1640134.1640147},
	abstract = {Creating an interpreter is a simple and fast way to implement a dynamic programming language. With this ease also come major drawbacks. Interpreters are significantly slower than compiled machine code because they have a high dispatch overhead and cannot perform optimizations. To overcome these limitations, interpreters are commonly combined with just-in-time compilers to improve the overall performance. However, this means that a just-in-time compiler has to be implemented for each language.

We explore the approach of taking an interpreter of a dynamic 
language and running it on top of an optimizing trace-based virtual machine, i.e., we run a guest {VM} on top of a host {VM.} The host {VM} uses trace recording to observe the guest {VM} executing the application program. Each recorded trace represents a sequence
of guest {VM} bytecodes corresponding to a given execution path
through the application program. The host {VM} optimizes and compiles these traces to machine code, thus eliminating the need for a custom just-in-time compiler for the guest {VM.} The guest {VM} only needs to provide basic  information about its interpreter loop to the
host {VM.}},
	booktitle = {{DLS}},
	publisher = {{ACM}},
	author = {Yermolovich, Alexander and Wimmer, Christian and Franz, Michael},
	year = {2009},
	keywords = {actionscript, dynamic languages, hierarchical virtual machines, trace compilation},
	pages = {79--88}
},

@inproceedings{chambers_efficient_1989,
	title = {An efficient implementation of {SELF} a dynamically-typed object-oriented language based on prototypes},
	volume = {24},
	url = {http://portal.acm.org/citation.cfm?id=74884},
	doi = {10.1145/74878.74884},
	abstract = {We have developed and implemented techniques that double the performance of dynamically-typed object-oriented languages. Our {SELF} implementation runs twice as fast as the fastest Smalltalk implementation, despite {SELF's} lack of classes and explicit variables. To compensate for the absence of classes, our system uses implementation-level maps to transparently group objects cloned from the same prototype, providing data type information and eliminating the apparent space overhead for prototype-based systems. To compensate for dynamic typing, user-defined control structures, and the lack of explicit variables, our system dynamically compiles multiple versions of a source method, each customized according to its receiver's map. Within each version the type of the receiver is fixed, and thus the compiler can statically bind and inline all messages sent to self. Message splitting and type prediction extract and preserve even more static type information, allowing the compiler to inline many other messages. Inlining dramatically improves performance and eliminates the need to hard-wire low-level methods such as +,==, and {ifTrue:.} Despite inlining and other optimizations, our system still supports interactive programming environments. The system traverses internal dependency lists to invalidate all compiled methods affected by a programming change. The debugger reconstructs inlined stack frames from compiler-generated debugging information, making inlining invisible to the {SELF} programmer.},
	booktitle = {{OOPSLA}},
	author = {Chambers, C. and Ungar, D. and {{E.} Lee}},
	year = {1989},
	keywords = {self, specialization}
},

@inproceedings{holzle_optimizing_1991,
	title = {Optimizing Dynamically-Typed Object-Oriented Languages With Polymorphic Inline Caches},
	isbn = {3-540-54262-0},
	url = {http://portal.acm.org/citation.cfm?id=679193&dl=ACM&coll=portal},
	booktitle = {{ECOOP}},
	publisher = {Springer-Verlag},
	author = {Hölzle, Urs and Chambers, Craig and Ungar, David},
	year = {1991}
},

@inproceedings{rigo_representation-based_2004,
	address = {Verona, Italy},
	title = {Representation-based just-in-time specialization and the {P}syco prototype for {P}ython},
	isbn = {1-58113-835-0},
	url = {http://portal.acm.org/citation.cfm?id=1014010},
	doi = {10.1145/1014007.1014010},
	abstract = {A powerful application of specialization is to remove interpretative overhead: a language can be implemented with an interpreter, whose performance is then improved by specializing it for a given program source. This approach is only moderately successful with very high level languages, where the operation of each single step can be highly dependent on run-time data and context. In the present paper, the Psyco prototype for the Python language is presented. It introduces two novel techniques. The first is just-in-time specialization, or specialization by need, which introduces the "unlifting" ability for a value to be promoted from run-time to compile-time during specialization -- the inverse of the lift operator of partial evaluation. Its presence gives an unusual and powerful perspective on the specialization process. The second technique is representations, a theory of data-oriented specialization generalizing the traditional specialization domains (i.e. the compile-time/run-time dichotomy).},
	booktitle = {{PEPM}},
	publisher = {{ACM}},
	author = {Rigo, Armin},
	year = {2004},
	keywords = {{JIT}, Python}
},

@inproceedings{sullivan_dynamic_2003,
	address = {San Diego, California},
	title = {Dynamic native optimization of interpreters},
	isbn = {1-58113-655-2},
	url = {http://portal.acm.org/citation.cfm?id=858570.858576},
	doi = {10.1145/858570.858576},
	abstract = {For domain specific languages, "scripting languages", dynamic languages, and for virtual machine-based languages, the most straightforward implementation strategy is to write an interpreter. A simple interpreter consists of a loop that fetches the next bytecode, dispatches to the routine handling that bytecode, then loops. There are many ways to improve upon this simple mechanism, but as long as the execution of the program is driven by a representation of the program other than as a stream of native instructions, there will be some "interpretive {overhead".There} is a long history of approaches to removing interpretive overhead from programming language implementations. In practice, what often happens is that, once an interpreted language becomes popular, pressure builds to improve performance until eventually a project is undertaken to implement a native Just In Time {(JIT)} compiler for the language. Implementing a {JIT} is usually a large effort, affects a significant part of the existing language implementation, and adds a significant amount of code and complexity to the overall code {base.In} this paper, we present an innovative approach that dynamically removes much of the interpreted overhead from language implementations, with minimal instrumentation of the original interpreter. While it does not give the performance improvements of hand-crafted native compilers, our system provides an appealing point on the language implementation spectrum.},
	booktitle = {Workshop on Interpreters, virtual machines and emulators},
	publisher = {{ACM}},
	author = {Sullivan, Gregory T. and Bruening, Derek L. and Baron, Iris and Garnett, Timothy and Amarasinghe, Saman},
	year = {2003}
}
