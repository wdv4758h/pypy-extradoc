%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{setspace}
\usepackage{listings}


\begin{document}

\conferenceinfo{WXYZ '05}{date, City.} 
\copyrightyear{2005} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Title Text}
\subtitle{Subtitle Text, if any}

\authorinfo{Name1}
           {Affiliation1}
           {Email1}
\authorinfo{Name2\and Name3}
           {Affiliation2/3}
           {Email2/3}

\maketitle

\begin{abstract}
This is the text of the abstract.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

\terms
term1, term2

\keywords
keyword1, keyword2

\section{Introduction}

The text of the paper begins here.

\subsection{Running Example}
\label{sub:example}

For the purpose of this paper, we are going to use a tiny interpreter for a dynamic language with
 a very simple object
model, that just supports an integer and a float type. The objects support only
two operations, \lstinline{add}, which adds two objects (promoting ints to floats in a
mixed addition) and \lstinline{is_positive}, which returns whether the number is greater
than zero. The implementation of \lstinline{add} uses classical Smalltalk-like
double-dispatching.
%These classes could be part of the implementation of a very
%simple interpreter written in RPython.
The classes can be seen in
Figure~\ref{fig:objmodel} (written in RPython).

\begin{figure}
\begin{lstlisting}[mathescape,basicstyle=\setstretch{1.05}\ttfamily\scriptsize]
class Base(object):
   pass

class BoxedInteger(Base):
   def __init__(self, intval):
      self.intval = intval

   def add(self, other):
      return other.add__int(self.intval)

   def add__int(self, intother):
      return BoxedInteger(intother + self.intval)

   def add__float(self, floatother):
      floatvalue = floatother + float(self.intval)
      return BoxedFloat(floatvalue)

   def is_positive(self):
      return self.intval > 0

class BoxedFloat(Base):
   def __init__(self, floatval):
      self.floatval = floatval

   def add(self, other):
      return other.add__float(self.floatval)

   def add__int(self, intother):
      floatvalue = float(intother) + self.floatval
      return BoxedFloat(floatvalue)

   def add__float(self, floatother):
      return BoxedFloat(floatother + self.floatval)

   def is_positive(self):
      return self.floatval > 0.0


def f(y):
   step = BoxedInteger(-1)
   while y.is_positive():
      y = y.add(step)
   return res
\end{lstlisting}
\caption{An ``Interpreter'' for a Tiny Dynamic Language Written in RPython}
\label{fig:objmodel}
\end{figure}

Using these classes to implement arithmetic shows the basic problem of a
dynamic language implementation. All the numbers are instances of either
\lstinline{BoxedInteger} or \lstinline{BoxedFloat}, therefore they consume space on the
heap. Performing many arithmetic operations produces lots of garbage quickly,
putting pressure on the garbage collector. Using double dispatching to
implement the numeric tower needs two method calls per arithmetic operation,
which is costly due to the method dispatch.

Let us now consider a simple ``interpreter'' function \lstinline{f} that uses the
object model (see the bottom of Figure~\ref{fig:objmodel}).
The loop in \lstinline{f} iterates \lstinline{y} times, and computes something in the process.
Simply running this function is slow, because there are lots of virtual method
calls inside the loop, one for each \lstinline{is_positive} and even two for each
call to \lstinline{add}. These method calls need to check the type of the involved
objects repeatedly and redundantly. In addition, a lot of objects are created
when executing that loop, many of these objects are short-lived.
The actual computation that is performed by \lstinline{f} is simply a sequence of
float or integer additions.


\begin{figure}
\begin{lstlisting}[mathescape,numbers = right,basicstyle=\setstretch{1.05}\ttfamily\scriptsize]
$l_0$($p_{0}$, $p_{1}$):
# inside f: y = y.add(step)
guard_class($p_{1}$, BoxedInteger)
    # inside BoxedInteger.add
    $i_{2}$ = get($p_{1}$, intval)
    guard_class($p_{0}$, BoxedInteger)
        # inside BoxedInteger.add__int
        $i_{3}$ = get($p_{0}$, intval)
        $i_{4}$ = int_add($i_{2}$, $i_{3}$)
        $p_{5}$ = new(BoxedInteger)
            # inside BoxedInteger.__init__
            set($p_{5}$, intval, $i_{4}$)
jump($l_0$, $p_{0}$, $p_{5}$)
\end{lstlisting}
\caption{An Unoptimized Trace of the Example Interpreter}
\label{fig:unopt-trace}
\end{figure}

If the function is executed using the tracing JIT, with \lstinline{y} being a
\lstinline{BoxedInteger}, the produced trace looks like the one of
Figure~\ref{fig:unopt-trace} (lines starting with a hash ``\#'' are comments).
The trace corresponds to one iteration of the while-loop in \lstinline{f}.

The operations in the trace are indented
corresponding to the stack level of the function that contains the traced
operation. The trace is in single-assignment form, meaning that each variable is
assigned a value exactly once. The arguments $p_0$ and $p_1$ of the loop correspond
to the live variables \lstinline{y} and \lstinline{res} in the while-loop of
the original function.

The label of the loop is $l_0$ and is used by the jump instruction to
identify it's jump target.

The operations in the trace correspond to the operations in the RPython program
in Figure~\ref{fig:objmodel}:

\begin{itemize}
    \item \lstinline{new} creates a new object.
    \item \lstinline{get} reads an attribute of an object.
    \item \lstinline{set} writes to an attribute of an object.
    \item \lstinline{guard_class} is a precise type check and precedes an
    (inlined) method call and is followed by the trace of the called method.
    \item \lstinline{int_add} and \lstinline{int_gt} are integer addition and
    comparison (``greater than''), respectively.
    \item \lstinline{guard_true} checks that a boolean is true.
\end{itemize}

Method calls in the trace are preceded by a \lstinline{guard_class}
operation, to check that the class of the receiver is the same as the one that
was observed during tracing.\footnote{\lstinline{guard_class}
performs a precise
class check, not checking for subclasses.} These guards make the trace specific
to the situation where \lstinline{y} is really a \lstinline{BoxedInteger}. When
the trace is turned into machine code and afterwards executed with
\lstinline{BoxedFloat}, the
first \lstinline{guard_class} instruction will fail and execution will continue
using the interpreter.

The trace shows the inefficiencies of \lstinline{f} clearly, if one looks at
the number of \lstinline{new}, \lstinline{set/get} and \lstinline{guard_class}
operations. The number of \lstinline{guard_class} operation is particularly
problematic, not only because of the time it takes to run them. All guards also
have additional information attached that makes it possible to return to the
interpreter, should the guard fail. This means that too many guard operations also
consume a lot of memory.

In the rest of the paper we will see how this trace can be optimized using
partial evaluation.

\section{Optimizations}
Before the trace is passed to a backend compiling it into machine code
it needs to be optimized to achieve better performance.
The focus of this paper
is loop invariant code motion. The goal of that is to move as many
operations as possible out of the loop making them executed only once
and not every iteration. This we propose to achieve by loop peeling. It
leaves the loop body intact, but prefixes it with one iteration of the
loop. This operation by itself will not achieve anything. But if it is
combined with other optimizations it can increase the effectiveness of
those optimizations. For many optimization of interest some care has
to be taken when they are combined with loop peeling. This is
described below by first explaining the loop peeling optimization
followed by a set of other optimizations and how they interact with
loop peeling.

\subsection{Loop peeling}
Loop peeling is achieved by inlining the trace at the end of
itself. The input arguments of the second iteration are replaced with
the jump arguments of the first iterations and then the arguments of all
the operations are updated to operate on the new input arguments. To
keep the single-assignment form new variables has to be introduced as
the results of all the operations. The first iteration of the loop
will end with a jump to the second iteration of the loop while the
second iteration will end with a jump to itself. This way the first
copy of the trace only be executed once while the second copy will be
used for every other iteration. The rationality here is that the
optimizations below typically will be able to optimize the second copy
more efficiently than the first. The trace from Figure~\ref{fig:unopt-trace} would
after this operation become the trace in Figure~\ref{fig:peeled-trace}.

\begin{figure}
\begin{lstlisting}[mathescape,numbers = right,basicstyle=\setstretch{1.05}\ttfamily\scriptsize]
$l_0$($p_{0}$, $p_{1}$):
# inside f: y = y.add(step)
guard_class($p_{1}$, BoxedInteger)
    # inside BoxedInteger.add
    $i_{2}$ = get($p_{1}$, intval)
    guard_class($p_{0}$, BoxedInteger)
        # inside BoxedInteger.add__int
        $i_{3}$ = get($p_{0}$, intval)
        $i_{4}$ = int_add($i_{2}$, $i_{3}$)
        $p_{5}$ = new(BoxedInteger)
            # inside BoxedInteger.__init__
            set($p_{5}$, intval, $i_{4}$)
jump($l_1$, $p_{0}$, $p_{5}$)

$l_1$($p_{0}$, $p_{5}$):
# inside f: y = y.add(step)
guard_class($p_{5}$, BoxedInteger)
    # inside BoxedInteger.add
    $i_{6}$ = get($p_{5}$, intval)
    guard_class($p_{0}$, BoxedInteger)
        # inside BoxedInteger.add__int
        $i_{7}$ = get($p_{0}$, intval)
        $i_{8}$ = int_add($i_{6}$, $i_{7}$)
        $p_{9}$ = new(BoxedInteger)
            # inside BoxedInteger.__init__
            set($p_{9}$, intval, $i_{8}$)
jump($l_1$, $p_{0}$, $p_{9}$)
\end{lstlisting}
\caption{An Unoptimized Trace of the Example Interpreter}
\label{fig:peeled-trace}
\end{figure}

When applying the following optimizations to this two iteration trace
some care has to taken as to how the jump arguments of both
iterations and the input arguments of the second iteration are
treated. It has to be ensured that the second iteration stays a proper
trace in the sens that the operations within it only operations on
variables that are either among the input arguments of the second iterations
or are produced within the second iterations. To ensure this we need
to introduce a bit of formalism. 

The original trace (prior too peeling) consists of three parts. 
A vector of input
variables, $I=\left(I_1, I_2, \cdots, I_{|I|}\right)$, a list of non
jump operations and a single
jump operation. The jump operation contains a vector of jump variables,
$J=\left(J_1, J_2, \cdots, J_{|J|}\right)$, that are passed as the input variables of the target loop. After
loop peeling there will be a second copy of this trace with input
variables equal to the jump arguments of the first copy, $J$, and jump
arguments $K$. Looking back at our example we have
\begin{equation}
  %\left\{
    \begin{array}{lcl}
      I &=& \left( p_0, p_1 \right) \\
      J &=& \left( p_0, p_5 \right) \\
      K &=& \left( p_0, p_9 \right) \\
    \end{array}
  %\right.
  .
\end{equation}
To construct the second iteration from the first we also need a
function, $m$, mapping the variables of the first iteration onto the
variables of the second. This function is constructed during the
inlining. It is initialized by mapping the input arguments, $I$, to
the jump arguments $J$,
\begin{equation}
  m\left(I_i\right) = J_i \ \text{for}\ i = 1, 2, \cdots |I| .
\end{equation}
In the example that means (XXX which notation do we prefer?)
\begin{equation}
  m(v) = 
  \left\{
    \begin{array}{lcl}
      p_0 &\text{if}& v=p_0 \\
      p_5 &\text{if}& v=p_1 \\
    \end{array}
  \right.
  .
\end{equation}
\begin{equation}
  %\left\{
    \begin{array}{lcl}
      m\left(p_0\right) &=& p_0 \\
      m\left(p_1\right) &=& p_5
    \end{array}
  %\right.
  .
\end{equation}
Each operation in the trace is inlined in the order they are
executed. To inline an operation with argument vector 
$A=\left(A_1, A_2, \cdots, A_{|A|}\right)$ producing the variable $v$
a new variable, $\hat v$ is introduced. The inlined operation will
produce $\hat v$ from the input arguments 
\begin{equation}
  \left(m\left(A_1\right), m\left(A_2\right), 
    \cdots, m\left(A_{|A|}\right)\right) . 
\end{equation}
Before the
next operation is inlined, $m$ is extend by making $m\left(v\right) = \hat
v$. After all the operations in the example have been inlined we have
\begin{equation}
  %\left\{
    \begin{array}{lcl}
      m\left(p_0\right) &=& p_0 \\
      m\left(p_1\right) &=& p_5 \\
      m\left(i_2\right) &=& i_6 \\
      m\left(i_3\right) &=& i_7 \\
      m\left(i_4\right) &=& i_8 \\
      m\left(p_5\right) &=& p_9 \\
    \end{array}
  %\right.
  .
\end{equation}

\subsection{Redundant guard removal}
No special concerns needs to be taken when implementing redundant
guard removal together with loop peeling. However the the guards from
the first iteration might make the guards of the second iterations
redundant and thus removed. So the net effect of combining redundant
guard removal with loop peeling is that guards are moved out of the
loop. The second iteraton of the example reduces to

\begin{lstlisting}[mathescape,numbers = right,basicstyle=\setstretch{1.05}\ttfamily\scriptsize]
$l_1$($p_{0}$, $p_{5}$):
# inside f: y = y.add(step)
    # inside BoxedInteger.add
    $i_{6}$ = get($p_{5}$, intval)
        # inside BoxedInteger.add__int
        $i_{7}$ = get($p_{0}$, intval)
        $i_{8}$ = int_add($i_{6}$, $i_{7}$)
        $p_{9}$ = new(BoxedInteger)
            # inside BoxedInteger.__init__
            set($p_{9}$, intval, $i_{8}$)
jump($l_1$, $p_{0}$, $p_{9}$)
\end{lstlisting}

The guard on $p_5$ on line 17 of Figure~\ref{fig:unopt-trace} can be
removed since $p_5$ is allocated on line 10 with a known class. The
guard on $p_0$ on line 20 can be removed since it is identical to the
guard on line 6.

\subsection{Heap caching}
The objective of heap caching is to remove \lstinline{get} and
\lstinline{set} operations whose results can be deduced from previous
\lstinline{get} and \lstinline{set} operations. Exact details of the
process are outside the scope of this paper We will here assume that
it works perfectly and only consider the interactions with loop
peeling.

The issue at hand is to keep the second iteration a proper
trace. Consider the \lstinline{get} operation on line 19 of
Figure~\ref{fig:unopt-trace}. The result of this operation can be
deduced to be $i_4$ from the \lstinline{set} operation on line
12. Also, the result of the \lstinline{get} operation on line 22 can
be deduced to be $i_3$ from the \lstinline{get} operation on line
8. The optimization will thus remove line 19 and 22 from the trace and
replace $i_6$ with $i_4$ and $i_7$ with $i_3$. 

After that, the second
iteration will no longer be proper as it operates on $i_3$ and $i_4$
which are not part of it. The solution is to extend the input
arguments, $J$, with those two variables. This will also extend the
jump arguments of the first iteration, which is also $J$. 
Implicitly that also extends the jump arguments of the second iteration, $K$,
since they are the inlined versions of $J$. That is the, $I$ has to
be replaced by $\hat I$ which is formed as a concatenation of $I$ and
$\left(i_3, i_4\right)$. At the same time $K$ has to be replaced by
$\hat K$ which is formed as a concatenation of $K$ and 
$\left(m\left(i_3\right), m\left(i_4\right)\right) = \left(i_7, i_8\right)$. 
The variable $i_7$ will then be replaced by $i_3$ by the heap caching
algorithm as it has removed the variable $i_7$. XXX: Maybe we should
replace $i_7=$get(...) with $i_7=i_3$ instead of removing it?

In general what is needed is for the heap optimizer is to keep track of
which variables from the first iterations it reuses in the second
iteration. It has to construct a vector of such variables $H$ which
can be used to update the input and jump arguments,
\begin{equation}
  \hat J = \left(J_1, J_2, \cdots, J_{|J|}, H_1, H_2, \cdots, H_{|H}\right)
\end{equation}
\begin{equation}
  \hat K = \left(K_1, K_2, \cdots, K_{|J|}, m(H_1), m(H_2), \cdots, m(H_{|H})\right)
  .
\end{equation}
In the optimized trace $I$ is replaced by $\hat I$ and $K$ by $\hat
K$. The trace from Figure~\ref{fig:unopt-trace} will be optimized into

\begin{lstlisting}[mathescape,numbers = right,basicstyle=\setstretch{1.05}\ttfamily\scriptsize]
$l_0$($p_{0}$, $p_{1}$):
# inside f: y = y.add(step)
guard_class($p_{1}$, BoxedInteger)
    # inside BoxedInteger.add
    $i_{2}$ = get($p_{1}$, intval)
    guard_class($p_{0}$, BoxedInteger)
        # inside BoxedInteger.add__int
        $i_{3}$ = get($p_{0}$, intval)
        $i_{4}$ = int_add($i_{2}$, $i_{3}$)
        $p_{5}$ = new(BoxedInteger)
            # inside BoxedInteger.__init__
            set($p_{5}$, intval, $i_{4}$)
jump($l_1$, $p_{0}$, $p_{5}$, $i_3$, $i_4$)

$l_1$($p_{0}$, $p_{5}$, $i_3$, $i_4$):
# inside f: y = y.add(step)
    # inside BoxedInteger.add
        # inside BoxedInteger.add__int
        $i_{8}$ = int_add($i_{4}$, $i_{3}$)
        $p_{9}$ = new(BoxedInteger)
            # inside BoxedInteger.__init__
            set($p_{9}$, intval, $i_{8}$)
jump($l_1$, $p_{0}$, $p_{9}$, $i_3$, $i_8$)
\end{lstlisting}

\subsection{Allocation removal}
By using escape analysis it is possible to identify objects that are
allocated within the loop but never escapes it. That is the object are
short lived and no references to them exists outside the loop. This
is performed by processing the operation from top to bottom and
optimistically removing every \lstinline{new} operation. Later on if
it is discovered that a reference to the object escapes the loop, the
\lstinline{new} operation is inserted at this point. All operations
(\lstinline{get} and \lstinline{set}) on the removed objects are also
removed and the optimizer needs to keep track of the value of all
attributes of the object.

Consider again the original unoptimized trace of
Figure~\label{fig:peeled-trace}. Line 10 contains the first
allocation. It is removed and $p_5$ is marked as virtual. This means
that it refers to an virtual object that was not yet
(and might never be) allocated. Line 12 sets the \lstinline{intval}
attribute of $p_5$. This operation is also removed and the optimizer
registers that the attribute \lstinline{intval} of $p_5$ is $i_4$.

When the optimizer reaches line 13 it needs to construct the
arguments for the \lstinline{jump} operation, which contains the virtual
reference $p_5$. This can be achieved by exploding $p_5$ into it's
attributes. In this case there is only one attribute and it's value is
$i_4$, which means the $p_5$ is replaced with $i_4$ in the jump
arguments. 

In the general case, each virtual in the jump arguments is exploded into a
vector of variables containing the values of all it's attributes. If some
of the attributes are themselves virtuals they are recursively exploded
to make the vector contain only non virtual variables. Some care has
to be taken to always place the attributes in the same order when
performing this explosion. Notation becomes somewhat simpler if also every non
virtual variable of the jump arguments is exploded into a vector. This will
be a vector containing the original variable only. To summarize, for
every variable, $J_k$, of the original jump arguments, $J$, let
\begin{equation}
  \tilde J^{\left(k\right)} = \left\{
      \begin{array}{ll}
        \left(J_k\right)  & \text{if $J_k$ is not virtual} \\
        H^{\left(k\right)} & \text{if $J_k$ is virtual}
      \end{array}
  \right.
  ,
\end{equation}
where $H^{\left(k\right)}$ is a vector containing all non virtual
attributes of $J_k$. The arguments of the optimized \lstinline{jump}
operation are constructed as the concatenation all the $\tilde J^{\left(k\right)}$ vectors,
\begin{equation}
  \hat J = \left( 
    \begin{array}{cccc}
      \tilde J^{\left(1\right)} & \tilde J^{\left(2\right)} & \cdots &
      \tilde J^{\left(|J|\right)} \\
    \end{array}
  \right)      
  .
\end{equation}
and the arguments of the \lstinline{jump} operation of the second
operation, $K$, are replaced by inlining $\hat J$, 
\begin{equation}
  \hat K = \left(m\left(\hat J_1\right), m\left(\hat J_1\right), 
                 \cdots, m\left(\hat J_{|\hat J|}\right)\right)
  .
\end{equation}
In the optimized trace $I$ is replaced by $\hat I$ and $K$ by $\hat
K$. The trace from Figure~\ref{fig:unopt-trace} will be optimized into

\begin{lstlisting}[mathescape,numbers = right,basicstyle=\setstretch{1.05}\ttfamily\scriptsize]
$l_0$($p_{0}$, $p_{1}$):
# inside f: y = y.add(step)
guard_class($p_{1}$, BoxedInteger)
    # inside BoxedInteger.add
    $i_{2}$ = get($p_{1}$, intval)
    guard_class($p_{0}$, BoxedInteger)
        # inside BoxedInteger.add__int
        $i_{3}$ = get($p_{0}$, intval)
        $i_{4}$ = int_add($i_{2}$, $i_{3}$)
            # inside BoxedInteger.__init__
jump($l_1$, $p_{0}$, $i_{4}$)

$l_1$($p_{0}$, $i_{4}$):
# inside f: y = y.add(step)
    # inside BoxedInteger.add
    guard_class($p_{0}$, BoxedInteger)
        # inside BoxedInteger.add__int
        $i_{7}$ = get($p_{0}$, intval)
        $i_{8}$ = int_add($i_{4}$, $i_{7}$)
            # inside BoxedInteger.__init__
jump($l_1$, $p_{0}$, $i_8$)
\end{lstlisting}

Note that virtuals are only exploded into their attributes when
constructing the arguments of the jump of the first iteration. This
explosion can't be repeated when constructing the arguments of the
jump of the second iteration as it has to mach the first. This means
the objects that was passed as pointers (non virtuals) from the first
iteration to the second also has to be passed as pointers from the
second iteration to the third. If one of these objects are virtual
at the end of the second iteration they need to be allocated right
before the jump. With the simple objects considered in this paper,
that is not a problem. However in more complicated interpreters such
an allocation might, in combination with other optimizations, lead
to additional variables from the first iteration being imported into
the second. This extends both $\hat J$ and $\hat K$, which means that
some care has to be taken, when implementing this, to allow $\hat J$ to
grow while inlining it into $\hat K$.

\section{Benchmarks}

\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}

\end{document}
